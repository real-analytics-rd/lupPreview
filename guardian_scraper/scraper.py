# AUTOGENERATED! DO NOT EDIT! File to edit: 00_scraper.ipynb (unless otherwise specified).

__all__ = ['Parser', 'PageExtractor', 'ScrapingTheGuardian', 'PreviewsMapping', 'Previews', 'MongoClient']

# Cell
import re
import dateparser
import pandas as pd
import numpy as np
import pymongo
import json
import random
from mongoengine import *
from typing import *
from requests_html import HTMLSession
from bs4 import BeautifulSoup
from time import sleep
from datetime import datetime
from os import listdir
from os.path import isfile, join

# Cell
class Parser:
    """
    A class to represent previews pages parser.

    ...

    Methods
    -------
    parse_page(page_url, session)
        returns the html format of the page.
    store_page_locally(page, page_url)
        save a given page in a local folder.
    get_next_page(page)
        returns the link of the following page and if it's the last page.
    """

    @staticmethod
    def parse_page(page_url: str, session: HTMLSession) -> BeautifulSoup:
        """
            returns the html format of the page.

        Parameters
        ----------
        page_url: str
            the url of the page
        session: requests_html.HTMLSession
            the scraper session

        Returns
        -------
        page: bs4.BeautifulSoup
              the html format of the page

        """
        # Request the url
        request = session.get(page_url)
        # Get the html document of the page
        page = BeautifulSoup(request.text, "html.parser")
        return page

    @staticmethod
    def store_page_locally(page: BeautifulSoup, page_url: str) -> None:
        """
            Save a given page in a local folder.

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page
        page_url: str
            the url of the page

        Returns
        -------
        None

        """
        # Get the preview url
        # Delete the "https://www." part
        # Replace "/" by "_"

        page_url = page_url.replace("https://www.", "").replace("/", "_")
        directory_path = ".//previews//"
        file_path = directory_path + page_url + ".html"
        # Create a file and save the html content of the page
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(str(page))
        # Close the file
        file.close()

    @staticmethod
    def get_next_page(page: BeautifulSoup) -> Tuple[str, bool]:
        """
            returns the link of the following page and if it's the last page.

        Parameters
        ----------
        page : bs4.BeautifulSoup
            the html format of the page

        Returns
        -------
        url: str
          the url of the next page
        last_page: bool
          True if it's the last page, False otherwise.

        """
        # If we are at the last page , last_page = True else last_page = False
        last_page = False
        # Pick up the pagination HTML part
        pagination_section = page.find("div", {"class": "pagination__list"})
        # If we don't find the "next" button (it's the last page)
        # We are in the last page
        if not page.find("a", {"rel": "next"}):
            # We pick up the number of the page and we return the link
            html_location = dict({"aria-label": "Current page"})
            page_number = page.find("span", html_location).text
            url = (
                "https://www.theguardian.com/football/series/match-previews?page="
                + page_number
            )
            last_page = True
            return url, last_page
        # If it's not the last page, we pick up the link of the following page
        else:
            url = page.find("a", {"rel": "next"})["href"]
            return url, last_page

# Cell
class PageExtractor:
    """
    A class to represent an information extractor from a football preview.

    ...

    Methods
    -------
    get_values_matching_regex(page, regex)
        return all matched patterns from a preview page.
    extract_teams_names(title)
        returns team names from the preview title.
    extract_text_authors(page)
        returns the text and author of the preview.
    extract_preview_date(page)
        returns the publication date of the preview.
    extract_match_infos(page, venue_regex, referee_regex, odds_regex)
        returns a football match information (venue,referee,odds).
    """

    @staticmethod
    def get_values_matching_regex(
        page: BeautifulSoup, regex: str
    ) -> Union[List[str], None]:
        """
        returns all matched patterns from a preview page.

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page
        regex: str
            the regex expression

        Returns
        -------
        result: list of str
          matched values of the regex expression, None otherwise

        """
        # All Information are located in the "p tag" of html
        # We pick up all the p tags
        # some previews in 2009 have a different html tags and classes
        all_p_tags_new_formats = page.find_all("p", {"class": "dcr-bixwrd"})
        all_p_tags_old_format = page.select("div > p")
        # if exist
        if all_p_tags_new_formats:
            paragraphs = all_p_tags_new_formats
        else:
            paragraphs = all_p_tags_old_format

        for paragraph in paragraphs:
            # We pick up the string values located in the paragraph
            # For "odds" information, "Evens" or "Evs" are replaced by 1-1
            pattern_odds = re.compile("Evens|Evs", re.IGNORECASE)
            section = pattern_odds.sub("1-1", paragraph.text)
            # To extract our information regex pattern
            # To ignore case sensitivity we use re.I
            pattern_returned_values = re.compile(regex, re.IGNORECASE)
            # If a regex match is found, we return the list of values.
            # otherwise, an empty array is returned.
            if pattern_returned_values.findall(section):
                matching_result = pattern_returned_values.findall(section)
                # remove empty tuples from the list
                # example of a matching_result value
                # [('12-5', '11-10', '23-10', '', '')]
                result = [element for element in matching_result[0] if element]
                return result
        return None

    @staticmethod
    def extract_teams_names(title: str) -> Dict[str, object]:
        """
        returns team names from the preview title.

        Parameters
        ----------
        title: str
            the title of the preview

        Returns
        -------
        names: dict of object

        """
        # 3 possible formats for previews title
        # For example:
        # {Squad Sheets: Team A v Team B} or
        # {{Team A v Team B : match preview}} or
        # {{Team A v Team B : Squad sheets}}
        # We remove text before or after team names
        pattern = re.compile(
            "Squad Sheets:|: Squad[\s]sheets|Squad sheets|Squad sheet:|: match preview",
            re.IGNORECASE,
        )
        preview_title = pattern.sub("", title).strip()
        # Names are located in the title of the preview
        # Home team
        try:
            home_team = preview_title.split(" v ")[0]
        except Exception as e:
            home_team = None
        # Away team
        try:
            away_team = preview_title.split(" v ")[1].split("\t")[
                0
            ]  # for some preview we find team A v Team B \t date
        except Exception as e:
            away_team = None
        # we return names
        names = dict({"home": home_team, "away": away_team})
        return names

    @staticmethod
    def extract_text_authors(page: BeautifulSoup) -> Dict[str, str]:
        """
        returns the text and author of the preview.

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page

        Returns
        -------
        preview_text_author: dict of str

        """
        # Preview may not have text and author,
        # We initialize author and text to 'n/a' (not available),
        author = None
        text = None
        # all items are stored in a p tag
        # Some previews in 2009 have different html tags and classes
        all_p_tags_new_formats = page.find_all("p", {"class": "dcr-bixwrd"})
        all_p_tags_old_format = page.select("div > p")
        # if exist
        if all_p_tags_new_formats:
            all_p_tags = all_p_tags_new_formats
        else:
            all_p_tags = all_p_tags_old_format

        # it's quite difficult to determine which section is the text
        # the length of the text is usually the longest
        # dictionnary to store each p and its length
        length_texts = {}
        for p in all_p_tags:
            section = p.text
            length_texts[p] = len(section)

        # we pick the section with the largest size
        possible_text_section = max(length_texts, key=length_texts.get)
        # We double-check and only select texts with a size greater than 160
        if len(possible_text_section.text) > 160:
            text_section = possible_text_section
            text = text_section.text
            # the author name is located inside the text section
            # it is located in the strong tag
            possible_author_section = text_section.find("strong")
            # for some previews the author information is not found
            # if it's available we take it , else it will be 'n/a'
            if str(possible_author_section) != "None":
                author = possible_author_section.text

        preview_text_author = dict({"text": text, "author": author})
        return preview_text_author

    @staticmethod
    def extract_preview_date(page: BeautifulSoup) -> Union[datetime, str]:
        """
          returns the publication date of the preview.

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page

        Returns
        -------
        preview_date: datetime.date
          if not found 'n/a'

        """
        # there are 2 dates for the preview
        # the first is the date of publication
        # the second is the date of the last modification which is hidden
        # we pick only the first one
        try:
            # Some preview in 2009 have different html tags and classes
            html_new_location = dict({"class": "dcr-km9fgb"})
            html_old_location = dict({"itemprop": "datePublished"})
            dates_section_new_format = page.find("div", html_new_location)
            dates_section_old_format = page.find("time", html_old_location)
            if dates_section_new_format:
                dates_section = dates_section_new_format.strings
            else:
                dates_section = dates_section_old_format.strings

            for date in dates_section:
                preview_date = dateparser.parse(date).date()
                break
        except Exception as e:
            preview_date = None

        return preview_date

    @staticmethod
    def extract_match_infos(
        page: BeautifulSoup, venue_regex: str, referee_regex: str, odds_regex: str
    ) -> Dict[str, str]:
        """
          returns a football match information (venue,referee,odds).

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page
        venue_regex: str
            venue regex expression
        referee_regex: str
            referee regex expression
        odds_regex: str
            odds regex expression

        Returns
        -------
        match_infos: dict of str

        """
        # Extract venue, referee and odds values
        try:
            venue = PageExtractor.get_values_matching_regex(page, venue_regex)[
                0
            ].strip()
        except Exception as e:
            venue = None
        try:
            referee = PageExtractor.get_values_matching_regex(page, referee_regex)[
                0
            ].strip()
        except Exception as e:
            referee = None

        odds = PageExtractor.get_values_matching_regex(page, odds_regex)

        match_infos = dict({"venue": venue, "referee": referee, "odds": odds})
        return match_infos


# Cell
class ScrapingTheGuardian:
    """
    A class to represent a scraper from the "Guardian" website.

    ...

    Attributes
    ----------
    session : requests_html.HTMLSession
        a web session
    VENUE_REGEX : str
        venue regex expression
    REFEREE_REGEX : str
        referee regex expression
    ODDS_REGEX : str
        odds regex expression

    Methods
    -------
    calculate_betting_odds(odds)
        returns decimal odds.
    extract_preview_items(page,title)
        returns all information of a football preview.
    save_previews_locally(self,page)
        save all browsed previews in a local folder.
    extract_previews_information(self,folder_path)
        returns all the information of all local previews.

    """

    # venue, referee, odds pattern regex
    # in some previews, all of the information is on the same line.
    VENUE_REGEX = "Venue(.*)Tickets|Venue(.*),|Venue(.*)"
    REFEREE_REGEX = "Referee(.*)This season's|Referee(.*)Last season's|Referee(.*)Odds|Referee(.*)|Ref(.*)Odds"
    # {Odds H 11-8 A 11-8 D 11-8}
    # {Odds Liverpool 11-8 Aston Villa 11-8 Draw 11-8}
    # missing label {Odds H 11-8 11-8 D 11-8}
    # missing value {Odds H 11-8 A 11-8}
    ODDS_REGEX = "Odds[\s]*.*[\s]+(\d{1,3}-\d{1,3})[\s]*.*[\s]+(\d{1,3}-\d{1,3})[\s]*.*[\s]+(\d{1,3}-\d{1,3})|Odds[\s]*.*[\s]+(\d{1,3}-\d{1,3})[\s]*.*[\s]+(\d{1,3}-\d{1,3})"

    def __init__(self):

        # Initialize session to start scraping
        self.session = HTMLSession()

    @staticmethod
    def calculate_betting_odds(odds: list) -> Dict[str, float]:
        """
          returns decimal odds.

        Parameters
        ----------
        odds: list of str
            odds values

        Returns
        -------
        betting_odds: dict of float

        """
        # Initialize betting odds to n/a (not available)
        # Some previews may not include odds
        odds_home = None
        odds_away = None
        odds_draw = None

        if odds is not None:  # If odds exist
            # example of odds:
            # {H 4-6 A 43-10 D 3-1}
            # {liverpool 4-6 Tottenham 43-10 Draw 3-1}
            # {H 4-6 43-10 D 3-1}
            # {H 4-6 A 43-10}
            # The formula will be (4/6)+1 , (43/10)+1 , (3/1)+1
            # Home team odds
            betting_odds_home = odds[0]
            try:
                odds_home = (
                    int(betting_odds_home.split("-")[0])
                    / int(betting_odds_home.split("-")[1])
                ) + 1
            except ZeroDivisionError:
                pass
            # Away team odds
            betting_odds_away = odds[1]
            try:
                odds_away = (
                    int(betting_odds_away.split("-")[0])
                    / int(betting_odds_away.split("-")[1])
                ) + 1
            except ZeroDivisionError:
                pass
            # if we have the normal format of odds
            # we will have 3 parts(odds_home,odds_away,odds_draw)
            if len(odds) == 3:
                # Draw odds
                betting_odds_draw = odds[2]
                try:
                    odds_draw = (
                        int(betting_odds_draw.split("-")[0])
                        / int(betting_odds_draw.split("-")[1])
                    ) + 1
                except ZeroDivisionError:
                    pass

        betting_odds = dict(
            {"odds_home": odds_home, "odds_away": odds_away, "odds_draw": odds_draw}
        )
        return betting_odds

    @staticmethod
    def extract_preview_items(
        page: BeautifulSoup, title: str, link: str
    ) -> Dict[str, object]:
        """
          returns all information of a football preview

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page
        title: str
            the title of the preview
        link: str
            the link of the preview

        Returns
        -------
        preview_items: dict of object

        """
        # meth1: extract team names
        names = PageExtractor.extract_teams_names(title)
        # Home team and  Away Team
        home_team = names["home"]
        away_team = names["away"]
        # meth2: extract match infos (venue,referee,odds)
        match_infos = PageExtractor.extract_match_infos(
            page,
            ScrapingTheGuardian.VENUE_REGEX,
            ScrapingTheGuardian.REFEREE_REGEX,
            ScrapingTheGuardian.ODDS_REGEX,
        )
        venue = match_infos["venue"]
        referee = match_infos["referee"]
        odds = match_infos["odds"]
        # meth3: extract text and author of the preview
        text_author = PageExtractor.extract_text_authors(page)
        text = text_author["text"]
        author = text_author["author"]
        # meth4: extract preview date
        preview_date = PageExtractor.extract_preview_date(page)
        # meth5: calculate betting odds
        betting_odds = ScrapingTheGuardian.calculate_betting_odds(odds)
        # Home team betting odds
        odds_home_team = betting_odds["odds_home"]
        # Away team betting odds
        odds_away_team = betting_odds["odds_away"]
        # Draw betting odds
        odds_draw = betting_odds["odds_draw"]
        # Return preview items
        preview_items = dict(
            {
                "home_team": home_team,
                "away_team": away_team,
                "text": text,
                "author": author,
                "venue": venue,
                "referee": referee,
                "odds": odds,
                "odds_home_team": odds_home_team,
                "odds_away_team": odds_away_team,
                "odds_draw": odds_draw,
                "preview_date": preview_date,
                "preview_link": link,
            }
        )
        return preview_items

    def save_previews_locally(self, page: BeautifulSoup) -> None:
        """
          save all browsed previews in local

        Parameters
        ----------
        page: bs4.BeautifulSoup
            the html format of the page

        Returns
        -------
        None

        """
        # We pick all of the match previews on the webpage.
        previews = page.findAll("div", {"class": "fc-item__content"})
        # for each preview we extract its information
        for preview in previews:
            preview_items = {}
            # Pick up the preview link
            preview_link = preview.find("a")["href"]
            print(preview_link)
            # Pick up the match preview page
            preview_page = Parser.parse_page(preview_link, self.session)
            # We need only Premier League Previews
            # To filter previews we need to Find the title of the preview
            # Champions league and Cups are not allowed
            preview_title = preview_page.find("h1").text
            # Check if "cup" or "Champions league" exists in:
            # title, link, preview topic section,preview aside section
            # we pick preview topic
            try:
                preview_topic = preview_page.find("div", {"class": "dcr-lwa3gj"}).text
            except Exception as e:
                # some previews in 2009 have different html tags
                preview_topic = preview_page.find("div", {"class": "submeta"}).text
            # we pick preview_aside
            try:
                preview_aside = preview_page.find(
                    "aside", {"data-gu-name": "title"}
                ).text
            except Exception as e:
                # some previews in 2009 have different html tags
                preview_aside = preview_page.find(
                    "div", {"class": "content__labels"}
                ).text
            # if the preview is not a cup or not for Champions league:
            # we proceed the extraction

            not_premier_league_found = False
            eliminated_matches = ["Champions League", "champions-league", "cup"]
            for word in eliminated_matches:
                # test if the word in the preview title
                if re.search(word, preview_title, re.IGNORECASE):
                    not_premier_league_found = True
                    break
                # test if the word in the preview link
                if re.search(word, preview_link, re.IGNORECASE):
                    not_premier_league_found = True
                    break
                # test if the word in the preview topic
                if re.search(word, preview_topic, re.IGNORECASE):
                    not_premier_league_found = True
                    break
                # test if the word in the preview aside
                if re.search(word, preview_aside, re.IGNORECASE):
                    not_premier_league_found = True
                    break
            # some previews include the type of competition in the text
            # we find FA Cup – Kick-off
            # so we want to eliminate these previews
            cup_in_text = PageExtractor.get_values_matching_regex(
                preview_page, "FA Cup – Kick-off"
            )

            if not not_premier_league_found and not cup_in_text:
                # save preview in a local folder
                Parser.store_page_locally(preview_page, preview_link)

    def extract_previews_information(self, folder_path: str) -> List[Dict[str, object]]:
        """
          returns all the information of all previews saved in a local folder

        Parameters
        ----------
        folder_path: str
            the local folder where previews are saved

        Returns
        -------
        List of(dict of object)

        """
        # get all previews files
        files = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]
        # a list to store previews information
        all_previews_information = []
        # We will proceed with the information extraction for each file (preview).
        for f in files:
            # get the preview path
            preview_path = join(folder_path, f)
            # open the file
            preview = open(preview_path)
            # get the preview link from the preview name
            # The preview's name is its link without the "https://www." and the extension html
            preview_link = (
                preview_path.replace(folder_path, "https://www.")
                .replace("_", "/")
                .replace(".html", "")
            )
            # get the html format of the preview using beautifulSoup
            preview_page = BeautifulSoup(preview, "html.parser")
            preview_title = preview_page.find("h1").text
            # get all information
            preview_infos = ScrapingTheGuardian.extract_preview_items(
                preview_page, preview_title, preview_link
            )
            # store information in the "all_previews_information" list
            all_previews_information.append(preview_infos)
            # just for testing
            print(preview_infos)
            print("-----------------------------------------------")

        return all_previews_information


# Cell
class PreviewsMapping:
    """
    A class to represent a data mapper from a mongo database.

    ...

    Methods
    -------
    get_team_id(team_name, df_teams)
        returns the "opta" ID of a given team.
    get_game_id_date(home_team_id, away_team_id, preview_date)
        returns the id and the date of a given game.
    get_mapped_data(data,df_teams)
        save all the mapped previews in a MongoDb collection.
    """

    @staticmethod
    def get_team_id(team_name: str, df_teams: pd.DataFrame) -> int:
        """
          returns the "opta" ID of a given team.

        Parameters
        ----------
        team_name: str
            the name of a given team
        df_teams: pd.DataFrame
            a dataframe that contains teams and their different names

        Returns
        -------
        int

        """
        # The name of a given team
        # Filter the dictionary
        # If the given team name exists in the dataframe
        # We return its optaID
        # Else we return -1
        team_name = team_name.strip().lower()
        df_filtred = df_teams[
            (df_teams["name"].str.lower() == team_name)
            | (df_teams["shortClubName"].str.lower() == team_name)
            | (df_teams["optaName"].str.lower() == team_name)
            | (df_teams["whoScoredName"].str.lower() == team_name)
            | (df_teams["sofifaName"].str.lower() == team_name)
            | (df_teams["statsName"].str.lower() == team_name)
            | (df_teams["inStatName"].str.lower() == team_name)
            | (df_teams["transfermarktName"].str.lower() == team_name)
            | (df_teams["fotmobName"].str.lower() == team_name)
            | (df_teams["oddsportalName"].str.lower() == team_name)
            | (df_teams["fminsideName"].str.lower() == team_name)
            | (df_teams["nickName1"].str.lower() == team_name)
            | (df_teams["nickName2"].str.lower() == team_name)
            | (df_teams["nickName3"].str.lower() == team_name)
        ]

        if len(df_filtred) > 0:
            return df_filtred["optaId"][df_filtred.index[0]]

        return -1

    @staticmethod
    def get_game_id_date(
        home_team_id: str, away_team_id: str, preview_date: datetime
    ) -> Dict[str, object]:
        """
          returns the id and the date of a given game.

        Parameters
        ----------
        home_team_id: str
            the opta id of a home team.
        away_team_id: str
            the opta id of an away team.

        Returns
        -------
        dict of object

        """
        # Initialize a MongoDb instance with mongoengine
        mongoengine_client = MongoClient.connect("0")
        # Do a MongoDb query
        # Filter data by gameDate,competitionId,
        # homeTeamId, awayTeamId
        # Query
        game_filter = {
            "gameDate": {"$gt": preview_date},
            "competitionId": 8,
            "homeTeamId": int(home_team_id),
            "awayTeamId": int(away_team_id),
        }
        # Get only gameId and gameDate fields
        projection = {"gameId": 1, "gameDate": 1, "_id": 0}
        # Get data
        result = mongoengine_client["opta"]["Fixture"].find_one(
            filter=game_filter, projection=projection
        )
        game_id = None
        game_date = None
        # If there is a match
        # We pick the game ID and date
        if result:
            game_id = result["gameId"]
            game_date = result["gameDate"]

        return dict({"gameId": game_id, "gameDate": game_date})

    @staticmethod
    def save_mapped_data(data: pd.DataFrame, df_teams: pd.DataFrame) -> None:
        """
          returns all the mapped information.

        Parameters
        ----------
        data: pd.DataFrame
            all previews information extracted from the Guardian.
        df_teams: pd.DataFrame
            a dataframe that contains teams and their different names.

        Returns
        -------
        None

        """
        # Copy previews data
        X = data.copy()
        X = X.replace({np.nan: None})
        # For each preview
        # We search home team and away team opta ID's
        # We pick the game ID and date from the opta.Fixture MongoDb collection
        for index, row in X.iterrows():
            # pick the home team name from the preview
            home_team = row["home_team"]
            # pick the away team name from the preview
            away_team = row["away_team"]
            # get their opta ID's
            home_team_id = PreviewsMapping.get_team_id(home_team, df_teams)
            away_team_id = PreviewsMapping.get_team_id(away_team, df_teams)
            # pick the preview date
            preview_date = dateparser.parse(row["preview_date"])
            # get the id and the date of the game
            game = PreviewsMapping.get_game_id_date(
                home_team_id, away_team_id, preview_date
            )
            # connect to our mongoDb cluster
            mongoengine_client = MongoClient.connect("1")
            # preview class
            preview = Previews(
                game_id=game["gameId"],
                home_team=row["home_team"],
                away_team=row["away_team"],
                text=row["text"],
                author=row["author"],
                venue=row["venue"],
                referee=row["referee"],
                odds=row["odds"],
                odds_home_team=row["odds_home_team"],
                odds_away_team=row["odds_away_team"],
                odds_draw=row["odds_draw"],
                game_date=game["gameDate"],
                preview_date=row["preview_date"],
                preview_link=row["preview_link"],
            )
            # Validate and save input raw data
            try:
                preview.validate()
                preview.save()
            except Exception as e:
                print("warning:", e)
                continue


# Cell
class Previews(Document):
    """
        A class to represent the extracted previews from the guardian.

    ...

    Attributes
    ----------
    game_id : int
        the opta game id
    home_team : str
        home team name
    away_team : str
        away_team name
    text : str
        preview text
    author : str
        preview author
    venue : str
        match venue
    referee : str
        match referee
    odds : str
        betting odds
    odds_home_team : float
        decimal betting odds for home team
    odds_away_team : float
        decimal betting odds for away team
    odds_draw : float
        decimal betting odds for draw
    game_date : datetime
        the date of the match
    preview_date : datetime
        the date of the preview
    preview_link : str
        the Guardian preview link

    """

    game_id = IntField()
    home_team = StringField()
    away_team = StringField()
    text = StringField()
    author = StringField()
    venue = StringField()
    referee = StringField()
    odds = StringField()
    odds_home_team = FloatField()
    odds_away_team = FloatField()
    odds_draw = FloatField()
    game_date = DateTimeField()
    preview_date = DateTimeField()
    preview_link = StringField()


# Cell
class MongoClient:
    """
    A class to represent a MongoDb client.

    ...

    Attributes
    ----------
    CREDENTIALS_PATH : str
        the file path of the MongoDb credentials

    Methods
    -------
    find_credentials()
        returns MongoDb credentials stored in a local file.
    connect(index)
        returns the mongoDb instance.
    """

    # the file path of the MongoDb credentials
    CREDENTIALS_PATH = "//home//meherkh//secrets//credentials.json"

    @staticmethod
    def find_credentials() -> dict:
        """
          returns MongoDb credentials stored in a local file.

        Returns
        -------
        dict

        """
        # open file and extract the json fields
        with open(MongoClient.CREDENTIALS_PATH) as credentials:
            mongo_credentials = json.load(credentials)
            return mongo_credentials

    @staticmethod
    def connect(index: str) -> pymongo.mongo_client.MongoClient:
        """
          returns the mongoDb instance.

        Parameters
        ----------
        index: str
            the index of the cluster

        Returns
        -------
        pymongo.mongo_client.MongoClient

        """
        # Initialize a MongoDb instance with mongoengine
        disconnect()
        DB_URI = MongoClient.find_credentials()["DB_URI"][index]
        mongoengine_client = connect(host=DB_URI)
        return mongoengine_client
