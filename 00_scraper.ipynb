{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian Scraper\n",
    "\n",
    "> Scraping Premier League Previews from the Guardian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200px\">\n",
    "    \n",
    "|            Issues                 |          Solutions          |\n",
    "|------------------------------     |-------------------|\n",
    "|   4 possible formats for previews(old format, new format,Cup's format and a particular format) |Select the appropriate html tags|\n",
    "|   Preview titles are not the same ( we can find Squad Sheets or match preview)|Pick only the names of the teams and eliminate the rest|\n",
    "|   The date of the match is not always available |Pick the preview date|\n",
    "|   The order of the elements and labels are not the same |Using regex patterns to get information|\n",
    "|   Missing values for betting odds |We treat the general case separately and we set up specific regex patterns for these particular cases|\n",
    "|   Odds format is different|We treat the general case separately and we set up specific regex patterns for these particular cases|\n",
    "|   We can find non-numeric values for Odds like (Evens,evens,Eve)|Replace evens by 1-1|\n",
    "|   There are some previews that don't have author and text|For previews that have no text, we put 'n/a' (not available)|\n",
    "|   The existence of previews for the FA CUP,Carabao Cup,Champions league,World Cup|Filter previews by title,link,topic,aside html section and preview text and allow only Premier League previews|\n",
    "|   We are not sure if the names of the teams are the same as the ones in Opta|Set up a dictionary or check manually to map teams to their IDs|\n",
    "|When we send many requests, the guardian server blocks your IP address, which is interpreted as a DDOS attack|Do a sleep of a random x seconds between requests or change your IP and work with rotating proxy|\n",
    "</div >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import mongoengine\n",
    "import json\n",
    "import random \n",
    "import logging\n",
    "from mongoengine import * \n",
    "from typing import *\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser Class\n",
    "\n",
    "##### This class is used to parse pages and has 3 functions:\n",
    "\n",
    "1- <b> parse_page </b> function:retrieves the html format of a given web page link.\n",
    "\n",
    "2- <b> store_page_locally </b> function: save a preview page in a specified local folder.\n",
    "\n",
    "3- <b> get_next_page </b> function: retrieves the link to the next page and determines if it is the last page of previews in order to stop scraping. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    A class to represent previews pages parser.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    parse_page(page_url, session)\n",
    "        returns the html format of the page.\n",
    "    store_page_locally(page, page_url)\n",
    "        save a given page in a local folder.\n",
    "    get_next_page(page)\n",
    "        returns the link of the following page and if it's the last page.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_page(page_url: str, session: HTMLSession) -> BeautifulSoup:\n",
    "        \"\"\"\n",
    "            returns the html format of the page.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page_url: str\n",
    "            the url of the page\n",
    "        session: requests_html.HTMLSession\n",
    "            the scraper session\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        page: bs4.BeautifulSoup\n",
    "              the html format of the page\n",
    "\n",
    "        \"\"\"\n",
    "        # Request the url\n",
    "        request = session.get(page_url)\n",
    "        # Get the html document of the page\n",
    "        page = BeautifulSoup(request.text, \"html.parser\")\n",
    "        return page\n",
    "\n",
    "    @staticmethod\n",
    "    def store_page_locally(page: BeautifulSoup, page_url: str) -> None:\n",
    "        \"\"\"\n",
    "            Save a given page in a local folder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        page_url: str\n",
    "            the url of the page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Get the preview url\n",
    "        # Delete the \"https://www.\" part\n",
    "        # Replace \"/\" by \"_\"\n",
    "\n",
    "        page_url = page_url.replace(\"https://www.\", \"\").replace(\"/\", \"_\")\n",
    "        directory_path = \".//previews//\"\n",
    "        file_path = directory_path + page_url + \".html\"\n",
    "        # Create a file and save the html content of the page\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(str(page))\n",
    "        # Close the file\n",
    "        file.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next_page(page: BeautifulSoup) -> Tuple[str, bool]:\n",
    "        \"\"\"\n",
    "            returns the link of the following page and if it's the last page.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page : bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        url: str\n",
    "          the url of the next page\n",
    "        last_page: bool\n",
    "          True if it's the last page, False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "        # If we are at the last page , last_page = True else last_page = False\n",
    "        last_page = False\n",
    "        # Pick up the pagination HTML part\n",
    "        pagination_section = page.find(\"div\", {\"class\": \"pagination__list\"})\n",
    "        # If we don't find the \"next\" button (it's the last page)\n",
    "        # We are in the last page\n",
    "        if not page.find(\"a\", {\"rel\": \"next\"}):\n",
    "            # We pick up the number of the page and we return the link\n",
    "            html_location = dict({\"aria-label\": \"Current page\"})\n",
    "            page_number = page.find(\"span\", html_location).text\n",
    "            url = (\n",
    "                \"https://www.theguardian.com/football/series/match-previews?page=\"\n",
    "                + page_number\n",
    "            )\n",
    "            last_page = True\n",
    "            return url, last_page\n",
    "        # If it's not the last page, we pick up the link of the following page\n",
    "        else:\n",
    "            url = page.find(\"a\", {\"rel\": \"next\"})[\"href\"]\n",
    "            return url, last_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageExtractor Class\n",
    "\n",
    "##### This class has five functions for extracting data from a given football preview:\n",
    "\n",
    "1- <b> get_values_matching_regex </b> returns values that match a regex expression.\n",
    "\n",
    "&emsp;Because the \"Guardian\" website has two possible formats, we defined two possible classifiers for the p tags <br>&emsp;containing the information to be extracted.<br>\n",
    "&emsp;We go through each p section, and if we find the result, we return it; otherwise, a None is returned.<br>\n",
    "&emsp;The result is a list of tuples, with each tuple representing a value that matches the regex pattern.<br> &emsp;Unsatisfied patterns for regexes that include <b>OR</b> conditions will be empty tuples. That's why you need to get rid of it.\n",
    "\n",
    "2- <b> extract_teams_names </b> returns the names of the two teams in a football preview.\n",
    "\n",
    "&emsp;The preview includes team names at the title level.\n",
    " <br>&emsp;example:\n",
    "          &emsp;&emsp;{{Squad Sheets: Team A v Team B}} \n",
    "         or &emsp;&emsp;{{Team A v Team B: match preview}} \n",
    "         or &emsp;&emsp;{{Team A v Team B: Squad Sheets}}\n",
    "<br>&emsp;As a result, our strategy is to delete the text preceding or following the names and recover each name <br>&emsp;individually.\n",
    "<br>&emsp;If we were successful in obtaining the names, they will be returned in a Python dictionary; <br>&emsp;otherwise, the values will be 'n/a'(Not available).\n",
    "\n",
    "3- <b> extract_text_authors </b>returns the text and author of a football preview.\n",
    "\n",
    "&emsp;It's difficult to determine the position of the text, but it's almost certainly the block with the most <br>&emsp;characters.\n",
    "<br>&emsp;To proceed, we store each paragraph and its size in a Python dictionary, and then we take the <br>&emsp;block with the largest size.\n",
    "<br>&emsp;To be sure, we double-check by only accepting texts with a size greater than 160 because there <br>&emsp;are football previews with no text or author.\n",
    "<br>&emsp;Furthermore, the author information is always under the text section, more specifically in a <br>&emsp;strong tag, so if the text does not exist, the author is missing as well.\n",
    "If we were successful in <br>&emsp;obtaining the text and the author, they will be returned in a Python dictionary. Otherwise, the <br>&emsp;values will be 'n/a'(Not available).\n",
    "\n",
    "4- <b> extract_preview_date </b> returns the date of publication of a football preview.\n",
    "\n",
    "&emsp;We have distinguished two dates for the date of publication: the first is the date of publication, <br>&emsp;and the second is the date of the most recent modification.\n",
    "In this sense, we go through the <br>&emsp;section where the two dates are located and take only the first and use 'dateparser' to convert <br>&emsp;the string into a date in \"yyyy-mm-dd\" format.\n",
    "If we were successful in obtaining the date, it will <br>&emsp; be returned. Otherwise, the value will be 'n/a'(Not available).\n",
    "\n",
    "5- <b> extract_match_infos </b> returns a football match information (venue, referee, odds).\n",
    "\n",
    "&emsp;Here, we'll call the first function <b>get_values_matching_regex</b> , which will allow us to retrieve this<br>&emsp;information by specifying a regex expression for each.<br>&emsp;If this data is not available, the value will be 'n/a'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PageExtractor:\n",
    "    \"\"\"\n",
    "    A class to represent an information extractor from a football preview.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    get_values_matching_regex(page, regex)\n",
    "        return all matched patterns from a preview page.\n",
    "    extract_teams_names(title)\n",
    "        returns team names from the preview title.\n",
    "    extract_text_authors(page)\n",
    "        returns the text and author of the preview.\n",
    "    extract_preview_date(page)\n",
    "        returns the publication date of the preview.\n",
    "    extract_match_infos(page, venue_regex, referee_regex, odds_regex)\n",
    "        returns a football match information (venue,referee,odds).\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_values_matching_regex(\n",
    "        page: BeautifulSoup, regex: str\n",
    "    ) -> Union[List[str], None]:\n",
    "        \"\"\"\n",
    "        returns all matched patterns from a preview page.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        regex: str\n",
    "            the regex expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result: list of str\n",
    "          matched values of the regex expression, None otherwise\n",
    "\n",
    "        \"\"\"\n",
    "        # All Information are located in the \"p tag\" of html\n",
    "        # We pick up all the p tags\n",
    "        # some previews in 2009 have a different html tags and classes\n",
    "        all_p_tags_new_formats = page.find_all(\"p\", {\"class\": \"dcr-bixwrd\"})\n",
    "        all_p_tags_old_format = page.select(\"div > p\")\n",
    "        # if exist\n",
    "        if all_p_tags_new_formats:\n",
    "            paragraphs = all_p_tags_new_formats\n",
    "        else:\n",
    "            paragraphs = all_p_tags_old_format\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # We pick up the string values located in the paragraph\n",
    "            # For \"odds\" information, \"Evens\" or \"Evs\" are replaced by 1-1\n",
    "            pattern_odds = re.compile(\"Evens|Evs\", re.IGNORECASE)\n",
    "            section = pattern_odds.sub(\"1-1\", paragraph.text)\n",
    "            # To extract our information regex pattern\n",
    "            # To ignore case sensitivity we use re.I\n",
    "            pattern_returned_values = re.compile(regex, re.IGNORECASE)\n",
    "            # If a regex match is found, we return the list of values.\n",
    "            # otherwise, an empty array is returned.\n",
    "            if pattern_returned_values.findall(section):\n",
    "                matching_result = pattern_returned_values.findall(section)\n",
    "                # remove empty tuples from the list\n",
    "                # example of a matching_result value\n",
    "                # [('12-5', '11-10', '23-10', '', '')]\n",
    "                result = [element for element in matching_result[0] if element]\n",
    "                return result\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_teams_names(title: str) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "        returns team names from the preview title.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        title: str\n",
    "            the title of the preview\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        names: dict of object\n",
    "\n",
    "        \"\"\"\n",
    "        # 3 possible formats for previews title\n",
    "        # For example:\n",
    "        # {Squad Sheets: Team A v Team B} or\n",
    "        # {{Team A v Team B : match preview}} or\n",
    "        # {{Team A v Team B : Squad sheets}}\n",
    "        # We remove text before or after team names\n",
    "        pattern = re.compile(\n",
    "            \"Squad Sheets:|: Squad[\\s]sheets|Squad sheets|Squad sheet:|: match preview\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "        preview_title = pattern.sub(\"\", title).strip()\n",
    "        # Names are located in the title of the preview\n",
    "        # Home team\n",
    "        try:\n",
    "            home_team = preview_title.split(\" v \")[0]\n",
    "        except Exception as e:\n",
    "            home_team = None\n",
    "        # Away team\n",
    "        try:\n",
    "            away_team = preview_title.split(\" v \")[1].split(\"\\t\")[\n",
    "                0\n",
    "            ]  # for some preview we find team A v Team B \\t date\n",
    "        except Exception as e:\n",
    "            away_team = None\n",
    "        # we return names\n",
    "        names = dict({\"home\": home_team, \"away\": away_team})\n",
    "        return names\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text_authors(page: BeautifulSoup) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        returns the text and author of the preview.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preview_text_author: dict of str\n",
    "\n",
    "        \"\"\"\n",
    "        # Preview may not have text and author,\n",
    "        # We initialize author and text to 'n/a' (not available),\n",
    "        author = None\n",
    "        text = None\n",
    "        # all items are stored in a p tag\n",
    "        # Some previews in 2009 have different html tags and classes\n",
    "        all_p_tags_new_formats = page.find_all(\"p\", {\"class\": \"dcr-bixwrd\"})\n",
    "        all_p_tags_old_format = page.select(\"div > p\")\n",
    "        # if exist\n",
    "        if all_p_tags_new_formats:\n",
    "            all_p_tags = all_p_tags_new_formats\n",
    "        else:\n",
    "            all_p_tags = all_p_tags_old_format\n",
    "\n",
    "        # it's quite difficult to determine which section is the text\n",
    "        # the length of the text is usually the longest\n",
    "        # dictionnary to store each p and its length\n",
    "        length_texts = {}\n",
    "        for p in all_p_tags:\n",
    "            section = p.text\n",
    "            length_texts[p] = len(section)\n",
    "\n",
    "        # we pick the section with the largest size\n",
    "        possible_text_section = max(length_texts, key=length_texts.get)\n",
    "        # We double-check and only select texts with a size greater than 160\n",
    "        if len(possible_text_section.text) > 160:\n",
    "            text_section = possible_text_section\n",
    "            text = text_section.text\n",
    "            # the author name is located inside the text section\n",
    "            # it is located in the strong tag\n",
    "            possible_author_section = text_section.find(\"strong\")\n",
    "            # for some previews the author information is not found\n",
    "            # if it's available we take it , else it will be 'n/a'\n",
    "            if str(possible_author_section) != \"None\":\n",
    "                author = possible_author_section.text\n",
    "\n",
    "        preview_text_author = dict({\"text\": text, \"author\": author})\n",
    "        return preview_text_author\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_preview_date(page: BeautifulSoup) -> Union[datetime, None]:\n",
    "        \"\"\"\n",
    "          returns the publication date of the preview.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preview_date: datetime.date\n",
    "          if not found None\n",
    "\n",
    "        \"\"\"\n",
    "        # there are 2 dates for the preview\n",
    "        # the first is the date of publication\n",
    "        # the second is the date of the last modification which is hidden\n",
    "        # we pick only the first one\n",
    "        try:\n",
    "            # Some preview in 2009 have different html tags and classes\n",
    "            html_new_location = dict({\"class\": \"dcr-km9fgb\"})\n",
    "            html_old_location = dict({\"itemprop\": \"datePublished\"})\n",
    "            dates_section_new_format = page.find(\"div\", html_new_location)\n",
    "            dates_section_old_format = page.find(\"time\", html_old_location)\n",
    "            if dates_section_new_format:\n",
    "                dates_section = dates_section_new_format.strings\n",
    "            else:\n",
    "                dates_section = dates_section_old_format.strings\n",
    "\n",
    "            for date in dates_section:\n",
    "                preview_date = dateparser.parse(date).date()\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logging.error('error: Preview date is not available')\n",
    "            preview_date = None\n",
    "\n",
    "        return preview_date\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_match_infos(\n",
    "        page: BeautifulSoup, venue_regex: str, referee_regex: str, odds_regex: str\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "          returns a football match information (venue,referee,odds).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        venue_regex: str\n",
    "            venue regex expression\n",
    "        referee_regex: str\n",
    "            referee regex expression\n",
    "        odds_regex: str\n",
    "            odds regex expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        match_infos: dict of str\n",
    "\n",
    "        \"\"\"\n",
    "        # Extract venue, referee and odds values\n",
    "        try:\n",
    "            venue = PageExtractor.get_values_matching_regex(page, venue_regex)[\n",
    "                0\n",
    "            ].strip()\n",
    "        except Exception as e:\n",
    "            logging.error('error: Venue information is not available')\n",
    "            venue = None\n",
    "        try:\n",
    "            referee = PageExtractor.get_values_matching_regex(page, referee_regex)[\n",
    "                0\n",
    "            ].strip()\n",
    "        except Exception as e:\n",
    "            logging.error('error: Referee information is not available')              \n",
    "            referee = None\n",
    "\n",
    "        odds = PageExtractor.get_values_matching_regex(page, odds_regex)\n",
    "\n",
    "        match_infos = dict({\"venue\": venue, \"referee\": referee, \"odds\": odds})\n",
    "        return match_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScrapingTheGuardian Class\n",
    "\n",
    "##### This class represents a scraper from the \"Guardian\" website and has 4 functions:\n",
    "\n",
    "1- <b> calculate_betting_odds </b> returns decimal odds.\n",
    "\n",
    "&emsp;In this section, we will calculate the odds derived from the football preview.\n",
    "<br>&emsp;Considering the following example:\n",
    "<br>&emsp;&emsp; [\"9-20\",\"29-5\",\"6-5\"] \n",
    "<br>&emsp;&emsp;We calculate each sport's rating separately using the following formula:\n",
    "<br>&emsp;&emsp;&emsp; home = (9/20) + 1 \n",
    "<br>&emsp;&emsp;&emsp; away = (29/5) + 1\n",
    "<br>&emsp;&emsp;&emsp; draw = (6/5) + 1\n",
    "<br>&emsp;If we were successful in obtaining decimal odds, they will be returned in a Python dictionary.<br>&emsp;Otherwise, the values will be 'n/a'(Not available).\n",
    "\n",
    "\n",
    "2- <b> extract_preview_items </b>returns the entire contents of a football preview.\n",
    "\n",
    "&emsp;In this section, we will call the functions defined in the PageExtractor class and return a Python dictionary containing all of this information.\n",
    "<br>&emsp;But first, we use the <b>calculate_betting_odds</b> function to calculate the sports odds for the home team's victory, the away team's victory, and a draw.\n",
    "\n",
    "\"home team\",\"away team\",\"text\",\"author\",\"venue\",\"referee\",\"odds\",\"odds home team\",\"odds away team\",\"odds draw\", \"preview date\",\"preview_link\" are the returned values.\n",
    "\n",
    "3- <b> save_previews_locally </b> save all browsed previews in a local folder.\n",
    "\n",
    "&emsp;we verify if we have reached the last extracted preview date,\n",
    "if yes, we will stop the scraper\n",
    "<br>&emsp;For a given page, we retrieve all the previews and go through them one by one, taking the link, title, subject, and aside section.\n",
    "<br>&emsp;if the words \"cup\" or \"champions league\" do not belong in these sections, we save the preview in a local folder\n",
    "<br>&emsp;Otherwise, we move on to the next preview.\n",
    "<br>&emsp;The <b>store_page_locally</b> function, will be called here to save the preview page.\n",
    "\n",
    "4- <b> extract_previews_information </b> returns the information of all local previews.\n",
    "\n",
    "&emsp;In a local folder, we extract the information for each file (preview in html format) by calling the <b> extract_preview_items</b> function <br>&emsp;and then we save the information for each preview in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ScrapingTheGuardian:\n",
    "    \"\"\"\n",
    "    A class to represent a scraper from the \"Guardian\" website.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    session : requests_html.HTMLSession\n",
    "        a web session\n",
    "    VENUE_REGEX : str\n",
    "        venue regex expression\n",
    "    REFEREE_REGEX : str\n",
    "        referee regex expression\n",
    "    ODDS_REGEX : str\n",
    "        odds regex expression\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    calculate_betting_odds(odds)\n",
    "        returns decimal odds.\n",
    "    extract_preview_items(page,title)\n",
    "        returns all information of a football preview.\n",
    "    save_previews_locally(self,page,last_date_stop,last_preview)\n",
    "        save all browsed previews in a local folder.\n",
    "    extract_previews_information(self,folder_path)\n",
    "        returns all the information of all local previews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # venue, referee, odds pattern regex\n",
    "    # in some previews, all of the information is on the same line.\n",
    "    VENUE_REGEX = \"Venue(.*)Tickets|Venue(.*),|Venue(.*)\"\n",
    "    REFEREE_REGEX = \"Referee(.*)This season's|Referee(.*)Last season's|Referee(.*)Odds|Referee(.*)|Ref(.*)Odds\"\n",
    "    # {Odds H 11-8 A 11-8 D 11-8}\n",
    "    # {Odds Liverpool 11-8 Aston Villa 11-8 Draw 11-8}\n",
    "    # missing label {Odds H 11-8 11-8 D 11-8}\n",
    "    # missing value {Odds H 11-8 A 11-8}\n",
    "    ODDS_REGEX = \"Odds[\\s]*.*[\\s]+(\\d{1,3}-\\d{1,3})[\\s]*.*[\\s]+(\\d{1,3}-\\d{1,3})[\\s]*.*[\\s]+(\\d{1,3}-\\d{1,3})|Odds[\\s]*.*[\\s]+(\\d{1,3}-\\d{1,3})[\\s]*.*[\\s]+(\\d{1,3}-\\d{1,3})\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize session to start scraping\n",
    "        self.session = HTMLSession()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_betting_odds(odds: list) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "          returns decimal odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        odds: list of str\n",
    "            odds values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        betting_odds: dict of float\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize betting odds to n/a (not available)\n",
    "        # Some previews may not include odds\n",
    "        odds_home = None\n",
    "        odds_away = None\n",
    "        odds_draw = None\n",
    "\n",
    "        if odds is not None:  # If odds exist\n",
    "            # example of odds:\n",
    "            # {H 4-6 A 43-10 D 3-1}\n",
    "            # {liverpool 4-6 Tottenham 43-10 Draw 3-1}\n",
    "            # {H 4-6 43-10 D 3-1}\n",
    "            # {H 4-6 A 43-10}\n",
    "            # The formula will be (4/6)+1 , (43/10)+1 , (3/1)+1\n",
    "            # Home team odds\n",
    "            betting_odds_home = odds[0]\n",
    "            try:\n",
    "                odds_home = (\n",
    "                    int(betting_odds_home.split(\"-\")[0])\n",
    "                    / int(betting_odds_home.split(\"-\")[1])\n",
    "                ) + 1\n",
    "            except ZeroDivisionError:\n",
    "                logging.error('error: Home team odds are wrong')\n",
    "                pass\n",
    "            # Away team odds\n",
    "            betting_odds_away = odds[1]\n",
    "            try:\n",
    "                odds_away = (\n",
    "                    int(betting_odds_away.split(\"-\")[0])\n",
    "                    / int(betting_odds_away.split(\"-\")[1])\n",
    "                ) + 1\n",
    "            except ZeroDivisionError:\n",
    "                logging.error('error: Away team odds are wrong')\n",
    "                pass\n",
    "            # if we have the normal format of odds\n",
    "            # we will have 3 parts(odds_home,odds_away,odds_draw)\n",
    "            if len(odds) == 3:\n",
    "                # Draw odds\n",
    "                betting_odds_draw = odds[2]\n",
    "                try:\n",
    "                    odds_draw = (\n",
    "                        int(betting_odds_draw.split(\"-\")[0])\n",
    "                        / int(betting_odds_draw.split(\"-\")[1])\n",
    "                    ) + 1\n",
    "                except ZeroDivisionError:\n",
    "                    logging.error('error: Draw odds are wrong')\n",
    "                    pass\n",
    "\n",
    "        betting_odds = dict(\n",
    "            {\"odds_home\": odds_home, \"odds_away\": odds_away, \"odds_draw\": odds_draw}\n",
    "        )\n",
    "        return betting_odds\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_preview_items(\n",
    "        page: BeautifulSoup, title: str, link: str\n",
    "    ) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "          returns all information of a football preview\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        title: str\n",
    "            the title of the preview\n",
    "        link: str\n",
    "            the link of the preview\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preview_items: dict of object\n",
    "\n",
    "        \"\"\"\n",
    "        # meth1: extract team names\n",
    "        names = PageExtractor.extract_teams_names(title)\n",
    "        # Home team and  Away Team\n",
    "        home_team = names[\"home\"]\n",
    "        away_team = names[\"away\"]\n",
    "        # meth2: extract match infos (venue,referee,odds)\n",
    "        match_infos = PageExtractor.extract_match_infos(\n",
    "            page,\n",
    "            ScrapingTheGuardian.VENUE_REGEX,\n",
    "            ScrapingTheGuardian.REFEREE_REGEX,\n",
    "            ScrapingTheGuardian.ODDS_REGEX,\n",
    "        )\n",
    "        venue = match_infos[\"venue\"]\n",
    "        referee = match_infos[\"referee\"]\n",
    "        odds = match_infos[\"odds\"]\n",
    "        # meth3: extract text and author of the preview\n",
    "        text_author = PageExtractor.extract_text_authors(page)\n",
    "        text = text_author[\"text\"]\n",
    "        author = text_author[\"author\"]\n",
    "        # meth4: extract preview date\n",
    "        preview_date = PageExtractor.extract_preview_date(page)\n",
    "        # meth5: calculate betting odds\n",
    "        betting_odds = ScrapingTheGuardian.calculate_betting_odds(odds)\n",
    "        # Home team betting odds\n",
    "        odds_home_team = betting_odds[\"odds_home\"]\n",
    "        # Away team betting odds\n",
    "        odds_away_team = betting_odds[\"odds_away\"]\n",
    "        # Draw betting odds\n",
    "        odds_draw = betting_odds[\"odds_draw\"]\n",
    "        # Return preview items\n",
    "        preview_items = dict(\n",
    "            {\n",
    "                \"home_team\": home_team,\n",
    "                \"away_team\": away_team,\n",
    "                \"text\": text,\n",
    "                \"author\": author,\n",
    "                \"venue\": venue,\n",
    "                \"referee\": referee,\n",
    "                \"odds\": odds,\n",
    "                \"odds_home_team\": odds_home_team,\n",
    "                \"odds_away_team\": odds_away_team,\n",
    "                \"odds_draw\": odds_draw,\n",
    "                \"preview_date\": preview_date,\n",
    "                \"preview_link\": link,\n",
    "            }\n",
    "        )\n",
    "        return preview_items\n",
    "\n",
    "    def save_previews_locally(\n",
    "        self, page: BeautifulSoup, last_date_stop: datetime, last_preview: bool\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "          save all browsed previews in local\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        last_date_stop : datetime\n",
    "            the last extracted preview in the database\n",
    "        last_preview: bool\n",
    "            an indicator to know when we should stop the scraper\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "\n",
    "        \"\"\"\n",
    "        # We pick all of the match previews on the webpage.\n",
    "        previews = page.findAll(\"div\", {\"class\": \"fc-item__content\"})\n",
    "        # for each preview we extract its information.\n",
    "\n",
    "        for preview in previews:\n",
    "            # we pick the preview date and we parse it in a date format\n",
    "            preview_date = preview.find(\"time\")[\"datetime\"]\n",
    "            preview_date = dateparser.parse(preview_date).date()\n",
    "            # if the date selected from the previews database exists\n",
    "            # and has been reached by the preview date, we stop the loop\n",
    "            # and mark last_preview as True.\n",
    "            if last_date_stop and preview_date <= last_date_stop.date():\n",
    "                logging.info(\n",
    "                    \"Finish: The Scraper is stopped. The last preview date is {}\".format(\n",
    "                        preview_date\n",
    "                    )\n",
    "                )\n",
    "                last_preview = True\n",
    "                break\n",
    "            # Pick up the preview link\n",
    "            preview_link = preview.find(\"a\")[\"href\"]\n",
    "            logging.info(\"Preview link: {}\".format(preview_link))\n",
    "            # Pick up the football match preview page\n",
    "            preview_page = Parser.parse_page(preview_link, self.session)\n",
    "            # We need only Premier League Previews\n",
    "            # To filter previews we need to Find the title of the preview\n",
    "            # Champions league and Cups are not allowed\n",
    "            preview_title = preview_page.find(\"h1\").text\n",
    "            # Check if \"cup\" or \"Champions league\" exists in:\n",
    "            # title, link, preview topic section,preview aside section\n",
    "            # we pick preview topic\n",
    "            try:\n",
    "                preview_topic = preview_page.find(\"div\", {\"class\": \"dcr-lwa3gj\"}).text\n",
    "            except Exception as e:\n",
    "                # some previews in 2009 have different html tags\n",
    "                preview_topic = preview_page.find(\"div\", {\"class\": \"submeta\"}).text\n",
    "            # we pick preview_aside\n",
    "            try:\n",
    "                preview_aside = preview_page.find(\n",
    "                    \"aside\", {\"data-gu-name\": \"title\"}\n",
    "                ).text\n",
    "            except Exception as e:\n",
    "                # some previews in 2009 have different html tags\n",
    "                preview_aside = preview_page.find(\n",
    "                    \"div\", {\"class\": \"content__labels\"}\n",
    "                ).text\n",
    "            # if the preview is not a cup or not for Champions league:\n",
    "            # we proceed the extraction\n",
    "\n",
    "            not_premier_league_found = False\n",
    "            eliminated_matches = [\"Champions League\", \"champions-league\", \"cup\"]\n",
    "            for word in eliminated_matches:\n",
    "                # test if the word in the preview title\n",
    "                if re.search(word, preview_title, re.IGNORECASE):\n",
    "                    not_premier_league_found = True\n",
    "                    break\n",
    "                # test if the word in the preview link\n",
    "                if re.search(word, preview_link, re.IGNORECASE):\n",
    "                    not_premier_league_found = True\n",
    "                    break\n",
    "                # test if the word in the preview topic\n",
    "                if re.search(word, preview_topic, re.IGNORECASE):\n",
    "                    not_premier_league_found = True\n",
    "                    break\n",
    "                # test if the word in the preview aside\n",
    "                if re.search(word, preview_aside, re.IGNORECASE):\n",
    "                    not_premier_league_found = True\n",
    "                    break\n",
    "            # some previews include the type of competition in the text\n",
    "            # we find FA Cup – Kick-off\n",
    "            # so we want to eliminate these previews\n",
    "            cup_in_text = PageExtractor.get_values_matching_regex(\n",
    "                preview_page, \"FA Cup – Kick-off\"\n",
    "            )\n",
    "\n",
    "            if not not_premier_league_found and not cup_in_text:\n",
    "                # save preview in a local folder\n",
    "                Parser.store_page_locally(preview_page, preview_link)\n",
    "\n",
    "        return last_preview\n",
    "\n",
    "    def extract_previews_information(self, folder_path: str) -> List[Dict[str, object]]:\n",
    "        \"\"\"\n",
    "          returns all the information of all previews saved in a local folder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_path: str\n",
    "            the local folder where previews are saved\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of(dict of object)\n",
    "\n",
    "        \"\"\"\n",
    "        # get all previews files\n",
    "        files = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "        # a list to store previews information\n",
    "        all_previews_information = []\n",
    "        # We will proceed with the information extraction for each file (preview).\n",
    "        for f in files:\n",
    "            # get the preview path\n",
    "            preview_path = join(folder_path, f)\n",
    "            # open the file\n",
    "            preview = open(preview_path)\n",
    "            # get the preview link from the preview name\n",
    "            # The preview's name is its link without the \"https://www.\" and the extension html\n",
    "            preview_link = (\n",
    "                preview_path.replace(folder_path, \"https://www.\")\n",
    "                .replace(\"_\", \"/\")\n",
    "                .replace(\".html\", \"\")\n",
    "            )\n",
    "            # get the html format of the preview using beautifulSoup\n",
    "            preview_page = BeautifulSoup(preview, \"html.parser\")\n",
    "            preview_title = preview_page.find(\"h1\").text\n",
    "            # get all information\n",
    "            preview_infos = ScrapingTheGuardian.extract_preview_items(\n",
    "                preview_page, preview_title, preview_link\n",
    "            )\n",
    "            # store information in the \"all_previews_information\" list\n",
    "            all_previews_information.append(preview_infos)\n",
    "            # just for testing\n",
    "            logging.info(\"Preview data: {}\".format(preview_infos))\n",
    "            logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "        return all_previews_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreviewsMapping Class\n",
    "\n",
    "##### This class represents a mapper from the \"Opta\" MongoDb database and has 3 functions:\n",
    "\n",
    "1- <b> get_team_id </b> returns the \"opta\" ID of a given team.\n",
    "\n",
    "&emsp;The use of a predefined dictionary containing the teams and their various names facilitates us in matching the team names extracted from the guardian and <br>&emsp;their IDs in the opta database. The use of a dictionary was required because the team names in the previews differ and sometimes use abbreviations or<br>&emsp;nicknames.\n",
    "\n",
    "2- <b> get_game_id_date </b>returns the id and the date of a given game.\n",
    "\n",
    "&emsp;After completing the scraping task and extracting the information, the previews must be matched with their ids in the opta.fixture database.\n",
    "We will query this <br>&emsp;database by specifying the home team, the away team, the closest gamedate to the preview publication date, and the Premier League competitionId, which  <br>&emsp;is equal to 8 in the database.\n",
    "\n",
    "3- <b> save_mapped_data </b> save all the mapped previews in a MongoDb collection.\n",
    "\n",
    "&emsp;For each preview in the data extracted from the Guardian, we will look for the id of the home team and the away team and match it with the <b>opta.fixture</b><br>&emsp;database to get the gameID and gameDate and finally we save it in a MongoDb collection.\n",
    "<br>&emsp;To complete our mission, we will call the last two functions <b> get_team_id </b> and <b>get_game_id_date</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PreviewsMapping:\n",
    "    \"\"\"\n",
    "    A class to represent a data mapper from a mongo database.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    get_team_id(team_name, df_teams)\n",
    "        returns the \"opta\" ID of a given team.\n",
    "    get_game_id_date(home_team_id, away_team_id, preview_date)\n",
    "        returns the id and the date of a given game.\n",
    "    get_mapped_data(data,df_teams)\n",
    "        save all the mapped previews in a MongoDb collection.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_team_id(team_name: str, df_teams: pd.DataFrame) -> int:\n",
    "        \"\"\"\n",
    "          returns the \"opta\" ID of a given team.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        team_name: str\n",
    "            the name of a given team\n",
    "        df_teams: pd.DataFrame\n",
    "            a dataframe that contains teams and their different names\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "\n",
    "        \"\"\"\n",
    "        # The name of a given team\n",
    "        # Filter the dictionary\n",
    "        # If the given team name exists in the dataframe\n",
    "        # We return its optaID\n",
    "        # Else we return -1\n",
    "        team_name = team_name.strip().lower()\n",
    "        df_filtred = df_teams[\n",
    "            (df_teams[\"name\"].str.lower() == team_name)\n",
    "            | (df_teams[\"shortClubName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"optaName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"whoScoredName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"sofifaName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"statsName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"inStatName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"transfermarktName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"fotmobName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"oddsportalName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"fminsideName\"].str.lower() == team_name)\n",
    "            | (df_teams[\"nickName1\"].str.lower() == team_name)\n",
    "            | (df_teams[\"nickName2\"].str.lower() == team_name)\n",
    "            | (df_teams[\"nickName3\"].str.lower() == team_name)\n",
    "        ]\n",
    "\n",
    "        if len(df_filtred) > 0:\n",
    "            return df_filtred[\"optaId\"][df_filtred.index[0]]\n",
    "\n",
    "        return -1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_game_id_date(\n",
    "        home_team_id: str, away_team_id: str, preview_date: datetime\n",
    "    ) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "          returns the id and the date of a given game.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        home_team_id: str\n",
    "            the opta id of a home team.\n",
    "        away_team_id: str\n",
    "            the opta id of an away team.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict of object\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize a MongoDb instance with mongoengine\n",
    "        mongoengine_client = MongoClient.connect(\"0\")\n",
    "        # Do a MongoDb query\n",
    "        # Filter data by gameDate,competitionId,\n",
    "        # homeTeamId, awayTeamId\n",
    "        # Query\n",
    "        game_filter = {\n",
    "            \"gameDate\": {\"$gt\": preview_date},\n",
    "            \"competitionId\": 8,\n",
    "            \"homeTeamId\": int(home_team_id),\n",
    "            \"awayTeamId\": int(away_team_id),\n",
    "        }\n",
    "        # Get only gameId and gameDate fields\n",
    "        projection = {\"gameId\": 1, \"gameDate\": 1, \"_id\": 0}\n",
    "        # Get data\n",
    "        result = MongoClient.find(\n",
    "            mongoengine_client, \"opta\", \"Fixture\", game_filter, projection\n",
    "        ).limit(1)\n",
    "        game_id = None\n",
    "        game_date = None\n",
    "        # If there is a match\n",
    "        # We pick the game ID and date\n",
    "        query = list(result)\n",
    "\n",
    "        if len(query) > 0:\n",
    "            game_id = query[0][\"gameId\"]\n",
    "            game_date = query[0][\"gameDate\"]\n",
    "\n",
    "        return dict({\"gameId\": game_id, \"gameDate\": game_date})\n",
    "\n",
    "    @staticmethod\n",
    "    def save_mapped_data(data: pd.DataFrame, df_teams: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "          returns all the mapped information.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pd.DataFrame\n",
    "            all previews information extracted from the Guardian.\n",
    "        df_teams: pd.DataFrame\n",
    "            a dataframe that contains teams and their different names.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Copy previews data\n",
    "        X = data.copy()\n",
    "        X = X.replace({np.nan: None})\n",
    "        # For each preview\n",
    "        # We search home team and away team opta ID's\n",
    "        # We pick the game ID and date from the opta.Fixture MongoDb collection\n",
    "        for index, row in X.iterrows():\n",
    "            # pick the home team name from the preview\n",
    "            home_team = row[\"home_team\"]\n",
    "            # pick the away team name from the preview\n",
    "            away_team = row[\"away_team\"]\n",
    "            # get their opta ID's\n",
    "            home_team_id = PreviewsMapping.get_team_id(home_team, df_teams)\n",
    "            away_team_id = PreviewsMapping.get_team_id(away_team, df_teams)\n",
    "            # pick the preview date\n",
    "            preview_date = dateparser.parse(row[\"preview_date\"])\n",
    "            # get the id and the date of the game\n",
    "            game = PreviewsMapping.get_game_id_date(\n",
    "                home_team_id, away_team_id, preview_date\n",
    "            )\n",
    "            logging.info('Game {} in {}: {} Vs {} '.format(game[\"gameId\"],preview_date,home_team, away_team))\n",
    "            # connect to our mongoDb cluster\n",
    "            mongoengine_client = MongoClient.connect(\"1\")\n",
    "            # preview class\n",
    "            preview = Previews(\n",
    "                gameId=game[\"gameId\"],\n",
    "                homeTeam=row[\"home_team\"],\n",
    "                awayTeam=row[\"away_team\"],\n",
    "                text=row[\"text\"],\n",
    "                author=row[\"author\"],\n",
    "                venue=row[\"venue\"],\n",
    "                referee=row[\"referee\"],\n",
    "                odds=row[\"odds\"],\n",
    "                oddsHomeTeam=row[\"odds_home_team\"],\n",
    "                oddsAwayTeam=row[\"odds_away_team\"],\n",
    "                oddsDraw=row[\"odds_draw\"],\n",
    "                gameDate=game[\"gameDate\"],\n",
    "                previewDate=row[\"preview_date\"],\n",
    "                previewLink=row[\"preview_link\"],\n",
    "            )\n",
    "            # Validate and save input raw data\n",
    "            MongoClient.save(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previews Class\n",
    "\n",
    "##### This class represents the extracted previews from the guardian:\n",
    "\n",
    "We created a class that contains the various attributes to store in order to save every preview information extracted from the Guardian website.\n",
    "<br>This class ensures that data is stored in a convenient format and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Previews(Document):\n",
    "    \"\"\"\n",
    "        A class to represent the extracted previews from the guardian.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    gameId : int\n",
    "        the opta game id\n",
    "    homeTeam : str\n",
    "        home team name\n",
    "    awayTeam : str\n",
    "        away_team name\n",
    "    text : str\n",
    "        preview text\n",
    "    author : str\n",
    "        preview author\n",
    "    venue : str\n",
    "        match venue\n",
    "    referee : str\n",
    "        match referee\n",
    "    odds : str\n",
    "        betting odds\n",
    "    oddsHomeTeam : float\n",
    "        decimal betting odds for home team\n",
    "    oddsAwayTeam : float\n",
    "        decimal betting odds for away team\n",
    "    oddsDraw : float\n",
    "        decimal betting odds for draw\n",
    "    gameDate : datetime\n",
    "        the date of the match\n",
    "    previewDate : datetime\n",
    "        the date of the preview\n",
    "    previewLink : str\n",
    "        the Guardian preview link\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    gameId = IntField()\n",
    "    homeTeam = StringField()\n",
    "    awayTeam = StringField()\n",
    "    text = StringField()\n",
    "    author = StringField()\n",
    "    venue = StringField()\n",
    "    referee = StringField()\n",
    "    odds = StringField()\n",
    "    oddsHomeTeam = FloatField()\n",
    "    oddsAwayTeam = FloatField()\n",
    "    oddsDraw = FloatField()\n",
    "    gameDate = DateTimeField()\n",
    "    previewDate = DateTimeField()\n",
    "    previewLink = StringField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoCLient Class\n",
    "\n",
    "##### This class represents a MongoDb Client and has 4 functions:\n",
    "\n",
    "1- <b> find_credentials </b> returns the MongoDb credentials stored in a local file.\n",
    "\n",
    "\n",
    "2- <b> connect </b>returns the MongoDb instance to connect to a given cluster.\n",
    "\n",
    "There are two database URIs in the credentials file, the first of which is for the \"OPTA\" Database.\n",
    "<br>The second is for another database that was created to store extracted previews.\n",
    "\n",
    "3- <b> save </b> save a MongoDb collection.\n",
    "\n",
    "4- <b> find </b> find a MongoDb query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MongoClient:\n",
    "    \"\"\"\n",
    "    A class to represent a MongoDb client.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    CREDENTIALS_PATH : str\n",
    "        the file path of the MongoDb credentials\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    find_credentials()\n",
    "        returns MongoDb credentials stored in a local file.\n",
    "    connect(index)\n",
    "        returns the mongoDb instance.\n",
    "    save(collection)\n",
    "        save the MongoDb collection.\n",
    "    find(mongoengine_client,db,collection,game_filter,projection)\n",
    "        find a MongoDb query.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # the file path of the MongoDb credentials\n",
    "    CREDENTIALS_PATH = \"//home//meherkh//secrets//credentials.json\"\n",
    "\n",
    "    @staticmethod\n",
    "    def find_credentials() -> dict:\n",
    "        \"\"\"\n",
    "          returns MongoDb credentials stored in a local file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "\n",
    "        \"\"\"\n",
    "        # open file and extract the json fields\n",
    "        with open(MongoClient.CREDENTIALS_PATH) as credentials:\n",
    "            mongo_credentials = json.load(credentials)\n",
    "            return mongo_credentials\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(index: str) -> pymongo.mongo_client.MongoClient:\n",
    "        \"\"\"\n",
    "          returns the mongoDb instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: str\n",
    "            the index of the cluster\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pymongo.mongo_client.MongoClient\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize a MongoDb instance with mongoengine\n",
    "        disconnect()\n",
    "        DB_URI = MongoClient.find_credentials()[\"DB_URI\"][index]\n",
    "        mongoengine_client = connect(host=DB_URI)\n",
    "        return mongoengine_client\n",
    "\n",
    "    @staticmethod\n",
    "    def save(\n",
    "        collection: mongoengine.base.metaclasses.TopLevelDocumentMetaclass,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "          save the MongoDb collection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        collection: mongoengine.base.metaclasses.TopLevelDocumentMetaclass\n",
    "            the MongoDb collection\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collection.validate()\n",
    "            collection.save()\n",
    "        except Exception as e:\n",
    "            logging.error('error: {}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def find(\n",
    "        mongoengine_client: pymongo.mongo_client.MongoClient,\n",
    "        db: str,\n",
    "        collection: str,\n",
    "        game_filter: dict,\n",
    "        projection: dict,\n",
    "    ) -> pymongo.cursor.Cursor:\n",
    "        \"\"\"\n",
    "          find a MongoDb query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mongoengine_client: mongoengine.base.metaclasses.TopLevelDocumentMetaclass\n",
    "            the MongoDb collection\n",
    "        db: str\n",
    "            the Database name\n",
    "        collection: str\n",
    "            the collection name\n",
    "        game_filter: dict\n",
    "            the query filter\n",
    "        projection: dict\n",
    "            the query projection\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pymongo.cursor.Cursor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        result = mongoengine_client[db][collection].find(\n",
    "            filter=game_filter, projection=projection\n",
    "        )\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping all pages and save in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting url\n",
    "url = \"https://www.theguardian.com/football/series/match-previews\"\n",
    "# initialize the scraper instance.\n",
    "scraper = ScrapingTheGuardian()\n",
    "# initially we are not at the last page.\n",
    "last_page = False\n",
    "# we'll extract the previews that haven't already been extracted.\n",
    "last_preview = False\n",
    "# we specify the last preview date in the previews collection on which the scraper will be turned off.\n",
    "mongoengine_client = MongoClient.connect(\"1\")\n",
    "projection = {\"previewDate\": 1, \"_id\": 0}\n",
    "result = (\n",
    "    MongoClient.find(mongoengine_client, \"opta\", \"previews\", {}, projection)\n",
    "    .sort(\"previewDate\", -1)\n",
    "    .sort(\"gameDate\", -1)\n",
    "    .limit(1)\n",
    ")\n",
    "# if the database is empty we will scrap all pages\n",
    "query = list(result)\n",
    "if len(query) > 0:\n",
    "    last_date_stop = query[0][\"previewDate\"]\n",
    "else:\n",
    "    last_date_stop = None\n",
    "\n",
    "# if we are not at the last page \n",
    "# and we haven't reached an extracted preview\n",
    "# we launch the scraper\n",
    "while not last_page and not last_preview:\n",
    "    # a random timer\n",
    "    time = random.randint(2, 60)\n",
    "    logging.info('Waiting for {} seconds ...'.format(time))\n",
    "    # wait time seconds\n",
    "    sleep(time)\n",
    "    logging.info('The current page URL: {}'.format(url))\n",
    "    # get the html format of the page containing previews\n",
    "    page = Parser.parse_page(url, scraper.session)\n",
    "    # launch the scraper , save previews in a local folder\n",
    "    # and get the first and last extracted preview date\n",
    "    # and if we are at the last preview or not\n",
    "    last_preview = scraper.save_previews_locally(page, last_date_stop, last_preview)\n",
    "    # get the url of the following page and verify if we are at the last page\n",
    "    url, last_page = Parser.get_next_page(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information from local previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meherkh/anaconda3/envs/Real_Analytics/lib/python3.7/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    }
   ],
   "source": [
    "local_folder = \".//previews//\"\n",
    "scraper = ScrapingTheGuardian()\n",
    "all_previews_information = scraper.extract_previews_information(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_previews_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>referee</th>\n",
       "      <th>odds</th>\n",
       "      <th>odds_home_team</th>\n",
       "      <th>odds_away_team</th>\n",
       "      <th>odds_draw</th>\n",
       "      <th>preview_date</th>\n",
       "      <th>preview_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bolton Wanderers</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Old Trafford, where they have not won since 19...</td>\n",
       "      <td>Tim Rich</td>\n",
       "      <td>Reebok Stadium</td>\n",
       "      <td>M Jones</td>\n",
       "      <td>[13-5, 10-11, 23-10]</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2009-10-02</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>Hull City</td>\n",
       "      <td>Hull's previous visit to Eastlands ended in a ...</td>\n",
       "      <td>Jamie Jackson</td>\n",
       "      <td>City of Manchester Stadium</td>\n",
       "      <td>L Probert</td>\n",
       "      <td>[2-9, 10-1, 21-5]</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>https://www.theguardian.com/football/2009/nov/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>Mark Hughes will have half an eye on events at...</td>\n",
       "      <td>Chris Bell</td>\n",
       "      <td>City of Manchester Stadium</td>\n",
       "      <td>K Friend</td>\n",
       "      <td>[2-5, 13-2, 3-1]</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2009-10-23</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Everton</td>\n",
       "      <td>Confidence could well be in short supply with ...</td>\n",
       "      <td>Louise Taylor</td>\n",
       "      <td>Stadium of Light</td>\n",
       "      <td>M Atkinson</td>\n",
       "      <td>[13-10, 9-5, 11-5]</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fulham</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Teams are increasingly viewing trips to Craven...</td>\n",
       "      <td>Dominic Fifield</td>\n",
       "      <td>Craven Cottage</td>\n",
       "      <td>S Bennett</td>\n",
       "      <td>[17-10, 27-20, 9-4]</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>Avram Grant is growing tired of the hypothetic...</td>\n",
       "      <td>James Callow</td>\n",
       "      <td>Fratton Park</td>\n",
       "      <td>P Dowd</td>\n",
       "      <td>[4-5, 3-1, 12-5]</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2009-12-04</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Bolton Wanderers</td>\n",
       "      <td>Bolton arrive at the DW Stadium outside the re...</td>\n",
       "      <td>Marcus Christenson</td>\n",
       "      <td>DW Stadium</td>\n",
       "      <td>A Wiley</td>\n",
       "      <td>[1-1, 12-5, 9-4]</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2009-12-18</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Blackburn Rovers</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>Surviving for three years is the key to Premie...</td>\n",
       "      <td>Richard Rae</td>\n",
       "      <td>Ewood Park</td>\n",
       "      <td>H Webb</td>\n",
       "      <td>[9-10, 13-5, 12-5]</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>https://www.theguardian.com/football/2009/nov/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Robbie Keane's belief that Tottenham have a sq...</td>\n",
       "      <td>Kevin McCarra</td>\n",
       "      <td>Emirates Stadium</td>\n",
       "      <td>M Clattenburg</td>\n",
       "      <td>[11-20, 9-2, 13-5]</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2009-10-30</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Jermain Defoe, Peter Crouch and Niko Kranjcar ...</td>\n",
       "      <td>Dominic Fifield</td>\n",
       "      <td>Fratton Park</td>\n",
       "      <td>P Dowd</td>\n",
       "      <td>[3-1, 4-5, 12-5]</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           home_team          away_team  \\\n",
       "0   Bolton Wanderers  Tottenham Hotspur   \n",
       "1    Manchester City          Hull City   \n",
       "2    Manchester City             Fulham   \n",
       "3         Sunderland            Everton   \n",
       "4             Fulham  Tottenham Hotspur   \n",
       "..               ...                ...   \n",
       "80        Portsmouth            Burnley   \n",
       "81    Wigan Athletic   Bolton Wanderers   \n",
       "82  Blackburn Rovers         Stoke City   \n",
       "83           Arsenal  Tottenham Hotspur   \n",
       "84        Portsmouth  Tottenham Hotspur   \n",
       "\n",
       "                                                 text              author  \\\n",
       "0   Old Trafford, where they have not won since 19...            Tim Rich   \n",
       "1   Hull's previous visit to Eastlands ended in a ...       Jamie Jackson   \n",
       "2   Mark Hughes will have half an eye on events at...          Chris Bell   \n",
       "3   Confidence could well be in short supply with ...       Louise Taylor   \n",
       "4   Teams are increasingly viewing trips to Craven...     Dominic Fifield   \n",
       "..                                                ...                 ...   \n",
       "80  Avram Grant is growing tired of the hypothetic...        James Callow   \n",
       "81  Bolton arrive at the DW Stadium outside the re...  Marcus Christenson   \n",
       "82  Surviving for three years is the key to Premie...         Richard Rae   \n",
       "83  Robbie Keane's belief that Tottenham have a sq...       Kevin McCarra   \n",
       "84  Jermain Defoe, Peter Crouch and Niko Kranjcar ...     Dominic Fifield   \n",
       "\n",
       "                         venue        referee                  odds  \\\n",
       "0               Reebok Stadium        M Jones  [13-5, 10-11, 23-10]   \n",
       "1   City of Manchester Stadium      L Probert     [2-9, 10-1, 21-5]   \n",
       "2   City of Manchester Stadium       K Friend      [2-5, 13-2, 3-1]   \n",
       "3             Stadium of Light     M Atkinson    [13-10, 9-5, 11-5]   \n",
       "4               Craven Cottage      S Bennett   [17-10, 27-20, 9-4]   \n",
       "..                         ...            ...                   ...   \n",
       "80                Fratton Park         P Dowd      [4-5, 3-1, 12-5]   \n",
       "81                  DW Stadium        A Wiley      [1-1, 12-5, 9-4]   \n",
       "82                  Ewood Park         H Webb    [9-10, 13-5, 12-5]   \n",
       "83            Emirates Stadium  M Clattenburg    [11-20, 9-2, 13-5]   \n",
       "84                Fratton Park         P Dowd      [3-1, 4-5, 12-5]   \n",
       "\n",
       "    odds_home_team  odds_away_team  odds_draw preview_date  \\\n",
       "0         3.600000        1.909091       3.30   2009-10-02   \n",
       "1         1.222222       11.000000       5.20   2009-11-27   \n",
       "2         1.400000        7.500000       4.00   2009-10-23   \n",
       "3         2.300000        2.800000       3.20   2009-12-24   \n",
       "4         2.700000        2.350000       3.25   2009-12-24   \n",
       "..             ...             ...        ...          ...   \n",
       "80        1.800000        4.000000       3.40   2009-12-04   \n",
       "81        2.000000        3.400000       3.25   2009-12-18   \n",
       "82        1.900000        3.600000       3.40   2009-11-27   \n",
       "83        1.550000        5.500000       3.60   2009-10-30   \n",
       "84        4.000000        1.800000       3.40   2009-10-16   \n",
       "\n",
       "                                         preview_link  \n",
       "0   https://www.theguardian.com/football/2009/oct/...  \n",
       "1   https://www.theguardian.com/football/2009/nov/...  \n",
       "2   https://www.theguardian.com/football/2009/oct/...  \n",
       "3   https://www.theguardian.com/football/2009/dec/...  \n",
       "4   https://www.theguardian.com/football/2009/dec/...  \n",
       "..                                                ...  \n",
       "80  https://www.theguardian.com/football/2009/dec/...  \n",
       "81  https://www.theguardian.com/football/2009/dec/...  \n",
       "82  https://www.theguardian.com/football/2009/nov/...  \n",
       "83  https://www.theguardian.com/football/2009/oct/...  \n",
       "84  https://www.theguardian.com/football/2009/oct/...  \n",
       "\n",
       "[85 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\".//datasets//previews.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary that contains the teams and their different names.\n",
    "df_dict = pd.read_csv(\".//datasets//final_data.csv\")\n",
    "# extracted previews\n",
    "df_guardian = pd.read_csv(\".//datasets//previews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>optaId</th>\n",
       "      <th>name</th>\n",
       "      <th>symid</th>\n",
       "      <th>shortClubName</th>\n",
       "      <th>optaName</th>\n",
       "      <th>whoScoredName</th>\n",
       "      <th>sofifaName</th>\n",
       "      <th>statsName</th>\n",
       "      <th>inStatName</th>\n",
       "      <th>transfermarktName</th>\n",
       "      <th>fotmobName</th>\n",
       "      <th>oddsportalName</th>\n",
       "      <th>fminsideName</th>\n",
       "      <th>nickName1</th>\n",
       "      <th>nickName2</th>\n",
       "      <th>nickName3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e78bbc137fd00c66162080bc9e987e67297643dc50616...</td>\n",
       "      <td>3</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>ARS</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal FC</td>\n",
       "      <td>Arsenal FC</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Arsenal FC</td>\n",
       "      <td>Arsenal Football Club</td>\n",
       "      <td>The Gunners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ef9883721814dd09038659130c61c76f18976cb7b8e86...</td>\n",
       "      <td>47</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>POR</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth</td>\n",
       "      <td>Portsmouth FC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Portsmouth FC</td>\n",
       "      <td>Portsmouth Football Club</td>\n",
       "      <td>Pompey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eb89c068ca204a72408360450847a990c97c5b5ff0ec9f...</td>\n",
       "      <td>110</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>STK</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke City</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>Stoke City FC</td>\n",
       "      <td>Stoke City Football Club</td>\n",
       "      <td>The Potters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c1a486f8ca465e58b6301f038e754058986187d454110c...</td>\n",
       "      <td>56</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>SUN</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Sunderland AFC</td>\n",
       "      <td>Sunderland AFC</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunderland FC</td>\n",
       "      <td>Sunderland Association Football Club</td>\n",
       "      <td>Sunderland A.F.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0db353094ccf93e0005cf378ea862b56e77cacc57b7c5e...</td>\n",
       "      <td>111</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>WIG</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Wigan Athletic</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wigan Athletic FC</td>\n",
       "      <td>Wigan Athletic Football Club</td>\n",
       "      <td>The Latics The Tics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ID  optaId            name  \\\n",
       "0  9e78bbc137fd00c66162080bc9e987e67297643dc50616...       3         Arsenal   \n",
       "1  0ef9883721814dd09038659130c61c76f18976cb7b8e86...      47      Portsmouth   \n",
       "2  eb89c068ca204a72408360450847a990c97c5b5ff0ec9f...     110      Stoke City   \n",
       "3  c1a486f8ca465e58b6301f038e754058986187d454110c...      56      Sunderland   \n",
       "4  0db353094ccf93e0005cf378ea862b56e77cacc57b7c5e...     111  Wigan Athletic   \n",
       "\n",
       "  symid shortClubName        optaName whoScoredName      sofifaName  \\\n",
       "0   ARS       Arsenal         Arsenal       Arsenal         Arsenal   \n",
       "1   POR    Portsmouth      Portsmouth    Portsmouth      Portsmouth   \n",
       "2   STK         Stoke      Stoke City         Stoke      Stoke City   \n",
       "3   SUN    Sunderland      Sunderland    Sunderland      Sunderland   \n",
       "4   WIG         Wigan  Wigan Athletic         Wigan  Wigan Athletic   \n",
       "\n",
       "        statsName      inStatName transfermarktName  fotmobName  \\\n",
       "0         Arsenal      Arsenal FC        Arsenal FC     Arsenal   \n",
       "1      Portsmouth      Portsmouth     Portsmouth FC         NaN   \n",
       "2      Stoke City           Stoke        Stoke City       Stoke   \n",
       "3      Sunderland  Sunderland AFC    Sunderland AFC  Sunderland   \n",
       "4  Wigan Athletic  Wigan Athletic    Wigan Athletic       Wigan   \n",
       "\n",
       "  oddsportalName fminsideName          nickName1  \\\n",
       "0        Arsenal      Arsenal         Arsenal FC   \n",
       "1            NaN          NaN      Portsmouth FC   \n",
       "2          Stoke        Stoke      Stoke City FC   \n",
       "3            NaN          NaN      Sunderland FC   \n",
       "4          Wigan          NaN  Wigan Athletic FC   \n",
       "\n",
       "                              nickName2            nickName3  \n",
       "0                 Arsenal Football Club          The Gunners  \n",
       "1              Portsmouth Football Club               Pompey  \n",
       "2              Stoke City Football Club          The Potters  \n",
       "3  Sunderland Association Football Club    Sunderland A.F.C.  \n",
       "4          Wigan Athletic Football Club  The Latics The Tics  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>referee</th>\n",
       "      <th>odds</th>\n",
       "      <th>odds_home_team</th>\n",
       "      <th>odds_away_team</th>\n",
       "      <th>odds_draw</th>\n",
       "      <th>preview_date</th>\n",
       "      <th>preview_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bolton Wanderers</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Old Trafford, where they have not won since 19...</td>\n",
       "      <td>Tim Rich</td>\n",
       "      <td>Reebok Stadium</td>\n",
       "      <td>M Jones</td>\n",
       "      <td>['13-5', '10-11', '23-10']</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2009-10-02</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>Hull City</td>\n",
       "      <td>Hull's previous visit to Eastlands ended in a ...</td>\n",
       "      <td>Jamie Jackson</td>\n",
       "      <td>City of Manchester Stadium</td>\n",
       "      <td>L Probert</td>\n",
       "      <td>['2-9', '10-1', '21-5']</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2009-11-27</td>\n",
       "      <td>https://www.theguardian.com/football/2009/nov/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>Mark Hughes will have half an eye on events at...</td>\n",
       "      <td>Chris Bell</td>\n",
       "      <td>City of Manchester Stadium</td>\n",
       "      <td>K Friend</td>\n",
       "      <td>['2-5', '13-2', '3-1']</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2009-10-23</td>\n",
       "      <td>https://www.theguardian.com/football/2009/oct/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Everton</td>\n",
       "      <td>Confidence could well be in short supply with ...</td>\n",
       "      <td>Louise Taylor</td>\n",
       "      <td>Stadium of Light</td>\n",
       "      <td>M Atkinson</td>\n",
       "      <td>['13-10', '9-5', '11-5']</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fulham</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Teams are increasingly viewing trips to Craven...</td>\n",
       "      <td>Dominic Fifield</td>\n",
       "      <td>Craven Cottage</td>\n",
       "      <td>S Bennett</td>\n",
       "      <td>['17-10', '27-20', '9-4']</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>https://www.theguardian.com/football/2009/dec/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          home_team          away_team  \\\n",
       "0  Bolton Wanderers  Tottenham Hotspur   \n",
       "1   Manchester City          Hull City   \n",
       "2   Manchester City             Fulham   \n",
       "3        Sunderland            Everton   \n",
       "4            Fulham  Tottenham Hotspur   \n",
       "\n",
       "                                                text           author  \\\n",
       "0  Old Trafford, where they have not won since 19...         Tim Rich   \n",
       "1  Hull's previous visit to Eastlands ended in a ...    Jamie Jackson   \n",
       "2  Mark Hughes will have half an eye on events at...       Chris Bell   \n",
       "3  Confidence could well be in short supply with ...    Louise Taylor   \n",
       "4  Teams are increasingly viewing trips to Craven...  Dominic Fifield   \n",
       "\n",
       "                        venue     referee                        odds  \\\n",
       "0              Reebok Stadium     M Jones  ['13-5', '10-11', '23-10']   \n",
       "1  City of Manchester Stadium   L Probert     ['2-9', '10-1', '21-5']   \n",
       "2  City of Manchester Stadium    K Friend      ['2-5', '13-2', '3-1']   \n",
       "3            Stadium of Light  M Atkinson    ['13-10', '9-5', '11-5']   \n",
       "4              Craven Cottage   S Bennett   ['17-10', '27-20', '9-4']   \n",
       "\n",
       "   odds_home_team  odds_away_team  odds_draw preview_date  \\\n",
       "0        3.600000        1.909091       3.30   2009-10-02   \n",
       "1        1.222222       11.000000       5.20   2009-11-27   \n",
       "2        1.400000        7.500000       4.00   2009-10-23   \n",
       "3        2.300000        2.800000       3.20   2009-12-24   \n",
       "4        2.700000        2.350000       3.25   2009-12-24   \n",
       "\n",
       "                                        preview_link  \n",
       "0  https://www.theguardian.com/football/2009/oct/...  \n",
       "1  https://www.theguardian.com/football/2009/nov/...  \n",
       "2  https://www.theguardian.com/football/2009/oct/...  \n",
       "3  https://www.theguardian.com/football/2009/dec/...  \n",
       "4  https://www.theguardian.com/football/2009/dec/...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_guardian.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meherkh/anaconda3/envs/Real_Analytics/lib/python3.7/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    }
   ],
   "source": [
    "X = df_guardian.copy()\n",
    "PreviewsMapping.save_mapped_data(X,df_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
