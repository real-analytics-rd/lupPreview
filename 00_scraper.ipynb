{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian Scraper\n",
    "\n",
    "> Scraping Premier League Previews from the Guardian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach the aim of our project, we have begun the first task, which is to collect football experts' comments and data from English Premier League matches.\n",
    "<br>For this task, we will use match previews from \"The Guardian.\" It goes back far enough, from 2009 to today, to allow us to integrate deep neural networks.\n",
    "<br>Indeed, \"The Guardian's\" football experts publish previews every week, usually two or three days before the matches.\n",
    "<br>In this regard, we began by creating a data extraction tool that will allow us to extract this information on a regular basis.\n",
    "<br>The information to be extracted is as follows:\n",
    "\n",
    "- The names of the competing teams.\n",
    "- The date of the game\n",
    "- The identity of the referee\n",
    "- The stadium's name\n",
    "- Sports odds that will be converted to decimal format\n",
    "- The football expert's text\n",
    "- The text's author.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A preliminary examination of the Guardian's website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200px\">\n",
    "    \n",
    "|            Issues                 |          Solutions          |\n",
    "|------------------------------     |-------------------|\n",
    "|   4 possible formats for previews(old format, new format,Cup's format and a particular format) |Select the appropriate html tags|\n",
    "|   Preview titles are not the same ( we can find Squad Sheets or match preview)|Pick only the names of the teams and eliminate the rest|\n",
    "|   The date of the match is not always available |Pick the preview date|\n",
    "|   The order of the elements and labels are not the same |Using regex patterns to get information|\n",
    "|   Missing values for betting odds |We treat the general case separately and we set up specific regex patterns for these particular cases|\n",
    "|   Odds format is different|We treat the general case separately and we set up specific regex patterns for these particular cases|\n",
    "|   We can find non-numeric values for Odds like (Evens,evens,Eve,odds-on)|Replace evens by 1-1|\n",
    "|   There are some previews that don't have author and text|For previews that have no text, we put None (not available)|\n",
    "|   The existence of previews for the FA CUP,Carabao Cup,Champions league,World Cup|Filter previews by checking if the match exists in \"Opta\" database, and pick only Premier League match |\n",
    "|   We are not sure if the names of the teams are the same as the ones in Opta|Set up a dictionary or check manually to map teams to their IDs|\n",
    "|When we send many requests, the guardian server blocks your IP address, which is interpreted as a DDOS attack|Do a sleep of a random x seconds between requests or change IP address and work with rotating proxy|\n",
    "</div >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import random\n",
    "import requests\n",
    "import logging\n",
    "from guardian_scraper.Db_Config import *\n",
    "from guardian_scraper.parser import *\n",
    "from guardian_scraper.extractor import *\n",
    "from guardian_scraper.mapper import *\n",
    "from guardian_scraper.models.preview import *\n",
    "from typing import Dict, Union\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScrapingTheGuardian Class\n",
    "\n",
    "##### This class represents a scraper from the \"Guardian\" website and has 4 functions:\n",
    "\n",
    "1- <b> calculate_betting_odds </b> returns decimal odds.\n",
    "\n",
    "&emsp;In this section, we will calculate the odds derived from the football preview.\n",
    "<br>&emsp;Considering the following example:\n",
    "<br>&emsp;&emsp; [\"9-20\",\"29-5\",\"6-5\"] \n",
    "<br>&emsp;&emsp;We calculate each sport's rating separately using the following formula:\n",
    "<br>&emsp;&emsp;&emsp; home = (9/20) + 1 \n",
    "<br>&emsp;&emsp;&emsp; away = (29/5) + 1\n",
    "<br>&emsp;&emsp;&emsp; draw = (6/5) + 1\n",
    "<br>&emsp;If we were successful in obtaining decimal odds, they will be returned in a Python dictionary.<br>&emsp;Otherwise, the values will be None (Not available).\n",
    "\n",
    "\n",
    "2- <b> extract_preview_items </b>returns the entire contents of a football preview.\n",
    "\n",
    "&emsp;In this section, we will call the functions defined in the PageExtractor class and return a Python dictionary containing all of this information.\n",
    "<br>&emsp;But first, we use the <b>calculate_betting_odds</b> function to calculate the sports odds for the home team's victory, the away team's victory, and a draw.\n",
    "\n",
    "\"game Id\",\"home team\",\"away team\",\"text\",\"author\",\"venue\",\"referee\",\"odds\",\"odds home team\",\"odds away team\",\"odds draw\", \"preview date\",\"game Date\",\"preview_link\" are the returned values.\n",
    "\n",
    "3- <b> extract_previews </b> returns the information of all extracted previews.\n",
    "\n",
    "&emsp;We retrieve all previews for a given page and go through them one by one, taking the link and getting its data using the \"Guardian Api.\"\n",
    "<br>&emsp;If the \"Guardian Api\" does not work, we will resort to the traditional process of html parsing. \n",
    "<br>&emsp;For each preview in the data extracted from the Guardian, we will look for the id of the home team and the away team and match it with the <b>opta.fixture</b><br>&emsp;database to get the gameID and gameDate and finally we save it in a MongoDb collection.\n",
    "<br>&emsp;If the game exists, we will proceed with the data extraction.\n",
    "The previous function, <b>extract_preview_items</b>, will be called here to extract information from <br>&emsp;each \n",
    "preview, which will then be stored in the list \"all previews.\"\n",
    "<br>&emsp;However, we will only extract previews that do not already exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ScrapingTheGuardian:\n",
    "    \"\"\"\n",
    "    A class to represent a scraper from the \"Guardian\" website.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    session : requests_html.HTMLSession\n",
    "        a web session\n",
    "    VENUE_REGEX : str\n",
    "        venue regex expression\n",
    "    REFEREE_REGEX : str\n",
    "        referee regex expression\n",
    "    ODDS_REGEX : str\n",
    "        odds regex expression\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    calculate_betting_odds(odds)\n",
    "        returns decimal odds.\n",
    "    extract_preview_items(content,link,preview_date,game_date,game_id,home_team,away_team)\n",
    "        returns all information of a football preview.\n",
    "    extract_previews(self,page,previews_last_date,last_preview,all_previews,df_teams)\n",
    "        returns the information of all extracted previews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # venue, referee, odds pattern regex\n",
    "    # in some previews, all of the information is on the same line.\n",
    "    VENUE_REGEX = \"Venue(.*)Tickets|Venue(.*),|Venue(.*)\"\n",
    "    REFEREE_REGEX = \"Referee(.*)This season|Referee(.*)Last season's|Referee(.*)Odds|Referee[\\s](.*)|Ref(.*)Odds\"\n",
    "    # {Odds H 11-8 A 11-8 D 11-8}\n",
    "    # {Odds Liverpool 11-8 Aston Villa 11-8 Draw 11-8}\n",
    "    # missing label {Odds H 11-8 11-8 D 11-8}\n",
    "    # missing value {Odds H 11-8 A 11-8}\n",
    "    ODDS_REGEX = \"Odds[\\s]*[a-zA-Z' ]*(\\d{1,3}-[\\s]*\\d{1,3})[\\s]*[a-zA-Z' ]*(\\d{1,3}-[\\s]*\\d{1,3})([\\s]*[a-zA-Z']*[\\s]*(\\d{1,3}-[\\s]*\\d{1,3}))*\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize session to start scraping\n",
    "        self.session = HTMLSession()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_betting_odds(odds: list) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "          returns decimal odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        odds: list of str\n",
    "            odds values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        betting_odds: dict of object\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize betting odds to n/a (not available)\n",
    "        # Some previews may not include odds\n",
    "        odds_home = None\n",
    "        odds_away = None\n",
    "        odds_draw = None\n",
    "\n",
    "        if odds is not None:  # If odds exist\n",
    "            # example of odds:\n",
    "            # {H 4-6 A 43-10 D 3-1}\n",
    "            # {liverpool 4-6 Tottenham 43-10 Draw 3-1}\n",
    "            # {H 4-6 43-10 D 3-1}\n",
    "            # {H 4-6 A 43-10}\n",
    "            # The formula will be (4/6)+1 , (43/10)+1 , (3/1)+1\n",
    "            # Home team odds\n",
    "            betting_odds_home = odds[0]\n",
    "            try:\n",
    "                odds_home = (\n",
    "                    int(betting_odds_home.split(\"-\")[0])\n",
    "                    / int(betting_odds_home.split(\"-\")[1])\n",
    "                ) + 1\n",
    "            except ZeroDivisionError:\n",
    "                logging.error(\"Home team odds are wrong\")\n",
    "                pass\n",
    "            # Away team odds\n",
    "            betting_odds_away = odds[1]\n",
    "            try:\n",
    "                odds_away = (\n",
    "                    int(betting_odds_away.split(\"-\")[0])\n",
    "                    / int(betting_odds_away.split(\"-\")[1])\n",
    "                ) + 1\n",
    "            except ZeroDivisionError:\n",
    "                logging.error(\"Away team odds are wrong\")\n",
    "                pass\n",
    "            # if we have the normal format of odds\n",
    "            # we will have 3 parts(odds_home,odds_away,odds_draw)\n",
    "            if len(odds) >= 3:\n",
    "                odds.pop(2)\n",
    "                # Draw odds\n",
    "                betting_odds_draw = odds[2]\n",
    "                try:\n",
    "                    odds_draw = (\n",
    "                        int(betting_odds_draw.split(\"-\")[0])\n",
    "                        / int(betting_odds_draw.split(\"-\")[1])\n",
    "                    ) + 1\n",
    "                except ZeroDivisionError:\n",
    "                    logging.error(\"Draw odds are wrong\")\n",
    "                    pass\n",
    "\n",
    "        betting_odds = dict(\n",
    "            {\"odds_home\": odds_home, \"odds_away\": odds_away, \"odds_draw\": odds_draw}\n",
    "        )\n",
    "        return betting_odds\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_preview_items(\n",
    "        content: BeautifulSoup,\n",
    "        link: str,\n",
    "        preview_date: datetime,\n",
    "        game_date: datetime,\n",
    "        game_id: int,\n",
    "        home_team: str,\n",
    "        away_team: str,\n",
    "        response_type: str,\n",
    "    ) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "          returns all information of a football preview\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        content: bs4.BeautifulSoup\n",
    "            the html format of the preview content\n",
    "        link: str\n",
    "            the link of the preview\n",
    "        preview_date: datetime\n",
    "            the preview date\n",
    "        game_date: datetime\n",
    "            the game date\n",
    "        game_id: int\n",
    "            the game id\n",
    "        home_team: str\n",
    "            the home team name\n",
    "        away_team: str\n",
    "            the away team name\n",
    "        response_type: str\n",
    "            the parsing method('api' or 'html')\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preview_items: dict of object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # meth1: extract match infos (venue,referee,odds)\n",
    "        match_infos = PageExtractor.extract_match_infos(\n",
    "            content,\n",
    "            response_type,\n",
    "            ScrapingTheGuardian.VENUE_REGEX,\n",
    "            ScrapingTheGuardian.REFEREE_REGEX,\n",
    "            ScrapingTheGuardian.ODDS_REGEX,\n",
    "        )\n",
    "        venue = match_infos[\"venue\"]\n",
    "        referee = match_infos[\"referee\"]\n",
    "        odds = match_infos[\"odds\"]\n",
    "        # meth2: extract text and author of the preview\n",
    "        text_author = PageExtractor.extract_text_authors(content, response_type)\n",
    "        text = text_author[\"text\"]\n",
    "        author = text_author[\"author\"]\n",
    "        # meth3: calculate betting odds\n",
    "        betting_odds = ScrapingTheGuardian.calculate_betting_odds(odds)\n",
    "        # Home team betting odds\n",
    "        odds_home_team = betting_odds[\"odds_home\"]\n",
    "        # Away team betting odds\n",
    "        odds_away_team = betting_odds[\"odds_away\"]\n",
    "        # Draw betting odds\n",
    "        odds_draw = betting_odds[\"odds_draw\"]\n",
    "        # Return preview items\n",
    "        preview_items = dict(\n",
    "            {\n",
    "                \"game_id\": game_id,\n",
    "                \"home_team\": home_team,\n",
    "                \"away_team\": away_team,\n",
    "                \"text\": text,\n",
    "                \"author\": author,\n",
    "                \"venue\": venue,\n",
    "                \"referee\": referee,\n",
    "                \"odds\": odds,\n",
    "                \"odds_home_team\": odds_home_team,\n",
    "                \"odds_away_team\": odds_away_team,\n",
    "                \"odds_draw\": odds_draw,\n",
    "                \"preview_date\": preview_date,\n",
    "                \"game_date\": game_date,\n",
    "                \"preview_link\": link,\n",
    "            }\n",
    "        )\n",
    "        return preview_items\n",
    "\n",
    "    def extract_previews(\n",
    "        self,\n",
    "        page: BeautifulSoup,\n",
    "        previews_last_date: datetime,\n",
    "        last_preview: bool,\n",
    "        all_previews: list,\n",
    "        df_teams: pd.DataFrame,\n",
    "        api_key: str,\n",
    "    ) -> Union[bool, list]:\n",
    "        \"\"\"\n",
    "          save all browsed previews in local\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        page: bs4.BeautifulSoup\n",
    "            the html format of the page\n",
    "        previews_last_date : datetime\n",
    "            the last extracted preview date in the database\n",
    "        last_preview: bool\n",
    "            an indicator to know when we should stop the scraper\n",
    "        all_previews: list\n",
    "            a list that contains all extracted previews\n",
    "        df_teams: pd.DataFrame\n",
    "            a dataframe that contains teams and their different names\n",
    "        api_key: str\n",
    "            the guardian api key\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "\n",
    "        \"\"\"\n",
    "        # We pick all of the match previews on the webpage.\n",
    "        previews = page.findAll(\"div\", {\"class\": \"fc-item__content\"})\n",
    "        # for each preview we extract its information.\n",
    "        for preview in previews:\n",
    "            # we pick the preview date and we parse it in a date format\n",
    "            preview_date = preview.find(\"time\")[\"datetime\"]\n",
    "            preview_date = dateparser.parse(preview_date, settings={\"TIMEZONE\": \"UTC\"})\n",
    "            # if the date selected from the previews database exists\n",
    "            # and has been reached by the preview date, we stop the loop\n",
    "            # and mark last_preview as True.\n",
    "            if previews_last_date and preview_date.date() <= previews_last_date.date():\n",
    "                logging.info(\"The scraper turned off\")\n",
    "                last_preview = True\n",
    "                break\n",
    "            # Pick the preview link\n",
    "            preview_link = preview.find(\"a\")[\"href\"]\n",
    "\n",
    "            # We extract the last part of the link, which corresponds to the preview api link\n",
    "            api_preview_url = preview_link.replace(\"https://www.theguardian.com\", \"\")\n",
    "            # request the api\n",
    "            response = requests.get(\n",
    "                \"https://content.guardianapis.com/\"\n",
    "                + api_preview_url\n",
    "                + \"?api-key=\"\n",
    "                + api_key\n",
    "                + \"&show-blocks=all\"\n",
    "            )\n",
    "            # if the api works we get the title and the content of the preview\n",
    "            # else we extract html contents\n",
    "            if response:\n",
    "                logging.info(\"The Guardian Api works\")\n",
    "                # get the preview data\n",
    "                data = response.json()\n",
    "                # preview title\n",
    "                preview_title = data[\"response\"][\"content\"][\"webTitle\"]\n",
    "                # preview content\n",
    "                preview_content = BeautifulSoup(\n",
    "                    data[\"response\"][\"content\"][\"blocks\"][\"body\"][0][\"bodyHtml\"],\n",
    "                    \"html.parser\",\n",
    "                )\n",
    "                # preview date\n",
    "                preview_date = data[\"response\"][\"content\"][\"webPublicationDate\"]\n",
    "                preview_date = dateparser.parse(\n",
    "                    preview_date, settings={\"TIMEZONE\": \"UTC\"}\n",
    "                )\n",
    "                response_type = \"api\"\n",
    "\n",
    "            else:\n",
    "                logging.info(\"The Guardian Api does not work\")\n",
    "                preview_content = Parser.parse_page(preview_link, self.session)\n",
    "                preview_title = preview_content.find(\"h1\").text\n",
    "                response_type = \"html\"\n",
    "\n",
    "            # extract team names\n",
    "            names = PageExtractor.extract_teams_names(preview_title)\n",
    "            # Home team and  Away Team\n",
    "            home_team = names[\"home\"]\n",
    "            away_team = names[\"away\"]\n",
    "            # get teams id\n",
    "            home_team_id = PreviewsMapping.get_team_id(home_team, df_teams)\n",
    "            away_team_id = PreviewsMapping.get_team_id(away_team, df_teams)\n",
    "            # pick the preview date\n",
    "            # get the id and the date of the game\n",
    "            game = PreviewsMapping.get_game_id_date(\n",
    "                home_team_id, away_team_id, preview_date\n",
    "            )\n",
    "            # if the game exists we extract the preview information\n",
    "            if game != None:\n",
    "                preview_infos = ScrapingTheGuardian.extract_preview_items(\n",
    "                    preview_content,\n",
    "                    preview_link,\n",
    "                    preview_date,\n",
    "                    game.gameDate,\n",
    "                    game.gameId,\n",
    "                    home_team,\n",
    "                    away_team,\n",
    "                    response_type,\n",
    "                )\n",
    "                logging.info(\"Returned Preview information: {}\".format(preview_infos))\n",
    "                # connect to database\n",
    "                mongoengine_client = MongoClient.connect(\"1\")\n",
    "                # preview class\n",
    "                preview = Previews(\n",
    "                    gameId=preview_infos[\"game_id\"],\n",
    "                    homeTeam=preview_infos[\"home_team\"],\n",
    "                    awayTeam=preview_infos[\"away_team\"],\n",
    "                    text=preview_infos[\"text\"],\n",
    "                    author=preview_infos[\"author\"],\n",
    "                    venue=preview_infos[\"venue\"],\n",
    "                    referee=preview_infos[\"referee\"],\n",
    "                    odds=preview_infos[\"odds\"],\n",
    "                    oddsHomeTeam=preview_infos[\"odds_home_team\"],\n",
    "                    oddsAwayTeam=preview_infos[\"odds_away_team\"],\n",
    "                    oddsDraw=preview_infos[\"odds_draw\"],\n",
    "                    gameDate=preview_infos[\"game_date\"],\n",
    "                    previewDate=preview_infos[\"preview_date\"],\n",
    "                    previewLink=preview_infos[\"preview_link\"],\n",
    "                )\n",
    "                # Validate and save input raw data\n",
    "                MongoClient.save(preview)\n",
    "                all_previews.append(preview_infos)\n",
    "\n",
    "            else:\n",
    "                logging.info(\n",
    "                    \"The game {} does not exist in the Opta database\".format(\n",
    "                        preview_title\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return last_preview, all_previews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
